<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Regression Basics</title>
  <meta name="description" content="">

  <link rel="stylesheet" href="/nhuyhoa//css/main.css">
  <link rel="canonical" href="http://yourdomain.com/nhuyhoa//2015/10/Regression-Basics.html">
  <link rel="alternate" type="application/rss+xml" title="Nhuy Hoa" href="http://yourdomain.com/nhuyhoa//feed.xml" />
</head>

  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/nhuyhoa//">Nhuy Hoa</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#ffffff" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#ffffff" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#ffffff" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/nhuyhoa//blog/">Blog</a>
          
        
          
          <a class="page-link" href="/nhuyhoa//categories/">Categories</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Regression Basics</h1>
    <p class="post-meta">Oct 20, 2015 • Categories: statistics </p>
  </header>

  <article class="post-content"> 
    <ul id="markdown-toc">
  <li><a href="#estimating-beta-coefficients" id="markdown-toc-estimating-beta-coefficients">Estimating Beta Coefficients</a></li>
  <li><a href="#estimating-variance-sum-square-errors" id="markdown-toc-estimating-variance-sum-square-errors">Estimating Variance: Sum Square Errors</a></li>
  <li><a href="#distribution-of-beta-estimates" id="markdown-toc-distribution-of-beta-estimates">Distribution of Beta Estimates</a></li>
  <li><a href="#regression-assumptions" id="markdown-toc-regression-assumptions">Regression Assumptions</a></li>
  <li><a href="#example" id="markdown-toc-example">Example</a></li>
</ul>

<p>The regression equation:</p>

<script type="math/tex; mode=display">Y = X\beta + \epsilon</script>

<h1 id="estimating-beta-coefficients">Estimating Beta Coefficients</h1>
<p>In least squares regression, we attemt to minimize the sum squared errors (SSE).</p>

<p>Let <script type="math/tex">Y</script> = observed responses and <script type="math/tex">\hat{Y}</script> = fitted responses. <script type="math/tex">\hat{Y}</script> lies on the column space of <script type="math/tex">X</script>, our design matrix. The idea is that there may not be a solution to <script type="math/tex">Y = X\beta</script>, so we project <script type="math/tex">Y</script> onto the <script type="math/tex">col(X)</script> in which we do have a solution.</p>

<p>In order to minimize the SSEs, we have <script type="math/tex">Y-\hat{Y}</script> is perpendicular to <script type="math/tex">col(X)</script>:</p>

<p><script type="math/tex">X^T(Y-\hat{Y}) = 0</script><br />
<script type="math/tex">X^T\hat{Y} = X^TY</script><br />
<script type="math/tex">X^TX\hat{\beta} = X^TY</script></p>

<script type="math/tex; mode=display">\hat{\beta} = (X^TX)^{-1}X^TY</script>

<h1 id="estimating-variance-sum-square-errors">Estimating Variance: Sum Square Errors</h1>
<p>The residual is <script type="math/tex">r = Y - \hat{Y} = Y - X\hat{\beta}</script>.</p>

<p>In least squares, the sum of the squared residuals are minimized.</p>

<p><script type="math/tex">SSE = r^Tr</script><br />
<script type="math/tex">= (Y - X\hat{\beta})^T(Y - X\hat{\beta})</script><br />
<script type="math/tex">= Y^TY - 2\hat{\beta}^TX^TY + \hat{\beta}^TX^TX\hat{\beta}</script><br />
<script type="math/tex">= Y^TY - \hat{\beta}^TX^TY + \hat{\beta}^T[X^TX\hat{\beta} - X^TY]</script></p>

<p>and since <script type="math/tex">X^TX\hat{\beta} = X^TY</script><br />
<script type="math/tex">= Y^TY - \hat{\beta}X^TY</script></p>

<p>So…</p>

<script type="math/tex; mode=display">SSE = Y^TY - \hat{\beta}^TX^TX\hat{\beta}</script>

<p>From this we can derive an unbiased estimate of <script type="math/tex">\sigma^2</script>, the mean square error:</p>

<script type="math/tex; mode=display">MSE = \frac{SSE}{n - p}</script>

<p>Note that in least squares, we always minimize the SSE. So the sum of the residuals is always equal to 0.</p>

<script type="math/tex; mode=display">min( \Sigma (y - \hat{y})^2 ) = 2 \Sigma (y - \hat{y}) = 0</script>

<h1 id="distribution-of-beta-estimates">Distribution of Beta Estimates</h1>
<p>We know that <br />
<script type="math/tex">E[Y] = X\beta</script><br />
<script type="math/tex">Var[Y] = \sigma^2</script></p>

<p>The beta parameters are unbiased:<br />
<script type="math/tex">E[\beta] = (X^TX)^{-1}X^TE[Y]</script><br />
<script type="math/tex">= (X^TX)^{-1}X^TX\beta</script></p>

<script type="math/tex; mode=display">E[\beta] = \beta</script>

<p>The variance of the beta parameters:<br />
<script type="math/tex">Var[\beta] = (X^TX)^{-1}X^TVar[Y]X(X^TX)^{-1}</script><br />
<script type="math/tex">= \sigma^2 (X^TX)^{-1}X^TX(X^TX)^{-1}</script></p>

<script type="math/tex; mode=display">Var[\beta] = \sigma^2 (X^TX)^{-1}</script>

<p>Thus <script type="math/tex">\hat{\beta}</script> ~ <script type="math/tex">N(\beta, \sigma^2(X^TX)^{-1})</script>, and since <script type="math/tex">\sigma^2</script> is estimated with <script type="math/tex">MSE</script>, we use a t-distribution to determine the sigificance of the <script type="math/tex">\hat{\beta}</script> parameter.</p>

<h1 id="regression-assumptions">Regression Assumptions</h1>
<p>Assumptions for OLS require <script type="math/tex">e_i</script> ~ <script type="math/tex">N(0, \sigma^2I)</script>, in other words:<br />
* Normal distribution of errors<br />
* Linear relationship<br />
* Constant variance of the errors <br />
* Independent, uncorrelated errors</p>

<p>These assumptions fullfill the requirement of the Gauss-Markov theorem.</p>

<p>The Gauss-Markov theorem states that if<br />
* <script type="math/tex">E[e_i] = 0</script><br />
* <script type="math/tex">Var[e_i] = \sigma^2I</script> - homoskedasticity<br />
* <script type="math/tex">cov[e_i, e_j] \forall i \ne j</script> - uncorrelated errors</p>

<p>then the <script type="math/tex">\hat{\beta}</script> derived above is the best linear unbiased estimator (BLUE) in that it has the lowest variance of all unbiased linear estimators.</p>

<h1 id="example">Example</h1>

<div class="highlight"><pre><code class="language-r" data-lang="r">y <span class="o">&lt;-</span> rnorm<span class="p">(</span><span class="m">100</span><span class="p">)</span>
x1 <span class="o">&lt;-</span> runif<span class="p">(</span><span class="m">100</span><span class="p">,</span> <span class="m">3</span><span class="p">,</span> <span class="m">7</span><span class="p">)</span>
x2 <span class="o">&lt;-</span> rexp<span class="p">(</span><span class="m">100</span><span class="p">,</span> <span class="m">2.2</span><span class="p">)</span>
x3 <span class="o">&lt;-</span> rpois<span class="p">(</span><span class="m">100</span><span class="p">,</span> <span class="m">1</span><span class="p">)</span>
X <span class="o">&lt;-</span> <span class="kp">as.matrix</span><span class="p">(</span><span class="kt">data.frame</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> x1<span class="p">,</span> x2<span class="p">,</span> x3<span class="p">))</span>
B <span class="o">&lt;-</span> <span class="kp">solve</span><span class="p">(</span><span class="kp">t</span><span class="p">(</span>X<span class="p">)</span> <span class="o">%*%</span> X<span class="p">)</span> <span class="o">%*%</span> <span class="kp">t</span><span class="p">(</span>X<span class="p">)</span> <span class="o">%*%</span> y
coef <span class="o">&lt;-</span> coef<span class="p">(</span>lm<span class="p">(</span>y <span class="o">~</span> x1 <span class="o">+</span> x2 <span class="o">+</span> x3<span class="p">))</span>
<span class="kt">c</span><span class="p">(</span>B<span class="p">)</span></code></pre></div>

<div class="highlight"><pre><code class="language-text" data-lang="text">## [1]  0.074280039 -0.006598314 -0.141762686 -0.015003000</code></pre></div>

<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="kt">c</span><span class="p">(</span>coef<span class="p">)</span></code></pre></div>

<div class="highlight"><pre><code class="language-text" data-lang="text">##  (Intercept)           x1           x2           x3 
##  0.074280039 -0.006598314 -0.141762686 -0.015003000</code></pre></div>


  </article>

</div>

        <p><br></p>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

  <p class = "foot-text">
    &copy; 2015 Jennifer Nguyen. Powered by Jekyll.<br>
    <a href="mailto:jennifernguyen1992@gmail.com">Email</a> | 
    <a href="https://www.linkedin.com/pub/jennifer-nguyen/b0/94/aa7" target="_blank">LinkedIn</a> | 
    <a href="https://github.com/jnguyen92" target="_blank">
      <span class="username">Github</span>
      <span class="icon  icon--github">
        <svg viewBox="0 0 16 16">
          <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
        </svg>
      </span>

    </a>
  </p>

  </div>
</footer>


  </body>


</html>
