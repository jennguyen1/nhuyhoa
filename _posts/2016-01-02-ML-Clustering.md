---
layout: post
title: "Unsupervised Learning: Clustering"
date: "January 2, 2016"
categories: ['statistics', 'machine learning']
---

* TOC
{:toc}



Unsupervised learning methods are employed when we only observe features and not the associated response variable. In these cases, we are not interested in prediction. The goal of unsupervised learning methods, such as clustering, is to find an informative way of visualizing data or discover subgroups among the observations. Thus, it is more of an exploratory technique.

**Advantages:**

* Easier/cheaper to obtain unlabeled data than labeled data

**Disadvantages:**

* Without labeled data, one would never truely know if result is "correct"
* No implicit goal (unlike in supervised learning where the goal is prediction)

# Distance Metrics

There are a variety of distance metrics that we can use. All metrics have the following properties

* $$dist(x_i, x_j) \ge 0$$
* $$dist(x_i, x_j) = 0$$ $$iff$$ $$x_i = x_j$$
* $$dist(x_i, x_j) = dist(x_j, x_i)$$
* $$dist(x_i, x_j) \le dist(x_i, x_k) + dist(x_k, x_j)$$

Common distance metrics include
* Euclidean distance: $$d(x_i, x_k) = \sqrt{\sum^p_{j = 1} (x_{ij} - x_{kj})^2}$$
* Manhattan distance: $$d(x_i, x_k) = \sqrt{\sum^p_{j = 1} \vert x_{ij} - x_{kj} \vert}$$
* Correlation based distance

# K-Means Clustering
With K-means clustering, the goal is to partition observations into a pre-specified number of clusters. With K-means clustering

* All observations belong to a cluster
* Clusters are non-overlapping; each observation belong to only 1 cluster
* $$K$$ is chosen via cross-validation or a penalized clustering objective

## Optimization Equation
This is done by minimizing the within cluster variation.

$$min_{C_1...C_K} \sum^K_{k = 1} WCV(C_k)$$

where $$WCV(C_k)$$ is the amount by which the observations within a cluster differs.

The measurement of $$WCV(C_k)$$ is based on the distance metric defined by the user.

If we were to use Euclidean distance, we would have
$$ WCV(C_k) = \sum_{i \in C_k} \sum^p_{j = 1} (x_{ij} - \bar{x}_{kj})^2$$

with the optimization problem being

$$ min_{C_1 ... C_K} \sum^K_{k = 1} \sum_{i \in C_k} \sum^p_{j = 1} (x_{ij} - \bar{x}_{kj})^2 $$

## Algorithm

1. Randomly assign a number, from $$1$$ to $$K$$, to each observation as the intial cluster assignments. Another option is to choose a $$K$$ random points to serve as the cluster centers and assign points to the nearest center. 
2. Iterate until cluster assignments consistent
  * For each of the $$K$$ clusters, compute the cluster centroid, the vector of the $$p$$ feature means for the observations in the $$k^{th}$$ cluster
  * Assign each observation to the cluster whose centroid is closest (based on the defined distance metric)

![K-means algorithm](http://jnguyen92.github.io/nhuyhoa/figure/images/cluster_kmeans_algorithm.png)
(Hastie, et.al)

This algorithm will find the local minimum for cluster assignments, but it is not guaranteed to find a global minimum. This is because we intially randomized the cluster assignments. 

To alleviate this problem, we can run the algorithm many times with random cluster assignments each time. In doing this, we can choose the replication that best miminizes our optimization problem and is (potentially) the global minimum. 

![K-means with random starts](http://jnguyen92.github.io/nhuyhoa/figure/images/cluster_kmeans_random_starts.png)
(Hastie, et.al)

# Gaussian Mixture Models
Gaussian mixture model clustering is a method similar to K-means clustering. However, this technique uses a soft clustering method, where each cluster is represented by a distribution. In this case, we assume the data is generated by a mixture of Gaussians. Similar to K-means clustering, the value of $$K$$ is chosen via cross-validation or a penalized clustering objective. 

Recall the Gaussian distribution

$$f_j (\overrightarrow{x_i}) = \frac{1}{\sqrt{(2\pi)^p \vert \Sigma_j \vert}} exp\left( -\frac{1}{2} (\overrightarrow{x_i} - \overrightarrow{\mu_j})^T \Sigma_j^{-1} (\overrightarrow{x_i} - \overrightarrow{\mu_j}) \right) $$

## EM Algorithm
The EM algorithm sets the paramters of the Gaussians $$\Theta$$ to maximize the log likelihood of the data $$X$$.

$$\log(likelihood(X\vert \Theta))$$
$$ = \log\prod^n_{i=1} P(\overrightarrow{x_i})$$
$$ = \log \prod^n_{i = 1} \sum^K_{k = 1} P_k(f_k(\overrightarrow{x_i}))$$
$$ = \sum^n_{i = 1} \log \sum^K_{k = 1} P_k(f_k(\overrightarrow{x_i}))$$

(We assume the covariance matrix is fixed and only adjust the means and weights) (Idk how to do the covariance matrix yet). 

The EM clustering algorithm involves

* Initialize parameters (means, cluster probabilities, etc)
* Loop until convergence using the E-step and the M-step

### Expectation Step
Let the hidden variable $$Z_{ij}$$ be $$1$$ if $$f_j$$ generated $$\overrightarrow{x_j}$$ and $$0$$ otherwise. The expectation of the hidden variable is 

$$h_{ij} = E(Z_{ij} = 1 \vert \overrightarrow{x_i}) = \frac{P_j(f_j(\overrightarrow{x_i}))}{\sum^K_{l = 1} P_l(f_l(\overrightarrow{x_i}))}$$

### Maximization Step
Given the expected values, we can re-estimate the means of the Gaussians and the cluster probabilities.

$$\overrightarrow{\mu_j} = \frac{\sum^n_{i = 1} h_{ij} \overrightarrow{x_i}}{\sum^n_{i = 1} h_{ij}} $$

$$P_j = \frac{\sum^n_{i = 1} h_{ij}}{n}$$

In this step, we may also re-estimate the covariance matrix if we were varying it (idk how yet).

# Hierarchical Clustering
With hierarchical clustering, the goal is to generate a tree-like visual representation of the observations (dendrogram) to view the clusterings obtained for each possible number of clusters. In this case, $$K$$ is not preset. 

## Algorithm
Hierarchical clustering uses a bottom-up clustering method to generate a dendeogram tree. This means that it grows a tree starting from the leaves. (Classification/regression trees use a top-down clustering method).

1. Begin with $$n$$ observations and a measure of all the $${n \choose 2}$$ pairwise dissimilarities. Add each cluster (individual observations) to the list of clusters to be joined
2. While number of clusters to be joined is greater than 1:
  * Examine all pairwise intercluster dissimilarities among the available clusters.
  * Fuse the pair of clusters that are most similar: remove pair of clusters and add its new fusion to list of clusters to be joined.
  * The dissimilarity b/n two clusters indicate the height of dendrogram at which fusion is placed.
  * Compute new pairwise intercluster dissimilarities among the $$i - 1$$ remaining clusters.

![hierarchical clustering algorithm](http://jnguyen92.github.io/nhuyhoa/figure/images/cluster_hierarchical_algorithm.png)

## Dissimilarity Metrics

* **Complete:**

$$dist(c_u, c_v) = min[dist(a, b) \vert a \in c_u, b \in c_v] $$

* **Single:**

$$dist(c_u, c_v) = max[dist(a, b) \vert a \in c_u, b \in c_v] $$

* **Average:**

$$dist(c_u, c_v) = avg[dist(a, b) \vert a \in c_u, b \in c_v] $$

![complete vs single linkages](http://jnguyen92.github.io/nhuyhoa/figure/images/cluster_complete_vs_single_link.png)

The type of linkage will affect the clustering results.

* Single linkage has the tendency to form long chains
* Complete linkage are sensitive to outliers
* Average linkage try to find a compromise between the two options

![Comparisons of Linkages](http://jnguyen92.github.io/nhuyhoa/figure/images/cluster_hierarchical_link_disadvantages.png)
(Hastie, et.al)

## Interpretting a Dendrogram

* Height of the fusion (vertical axis) indicates how different the observations (lower is more similar)
* Horizontal axis has no meaning

![dendrogram](http://jnguyen92.github.io/nhuyhoa/figure/images/cluster_hierarchical_dendrogram.png)

* Draw a horizontal line across a dendrogram to identify the clusters

![dendrogram cutting for clusters](http://jnguyen92.github.io/nhuyhoa/figure/images/cluster_hierarchical_cutting.png)


# Evaluation of Clusters

**Internal Validation:**

* Assess how well clustering optimizes the intracluster similarity and intercluster dissimilarity

The Silhouette index:
$$\frac{1}{K} \sum^K_{j = 1} \frac{1}{\vert C_j \vert} \sum_{x_i \in C_j} \frac{b(x_i) - a(x_i)}{max[b(x_i), a(x_i)]}$$

where $$b(x_i)$$ is the average distance of $$x_i$$ to instances in the next closest cluster and $$a(x_i)$$ is the average distance of $$x_i$$ is the average distance of $$x_i$$ to other instances in the same cluster. 

The more positive the SI, the better the clusters. 

**External Validation:**

* Use external/domain knowledge to assess clusters
