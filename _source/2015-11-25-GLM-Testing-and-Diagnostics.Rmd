---
layout: post
title: "GLM: Testing & Diagnostics"
date: "November 25, 2015"
categories: ['statistics', 'regression analysis']
---

* TOC
{:toc}

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(jn.general)
lib(data, viz)
knitr::opts_chunk$set(fig.width = 5, fig.height = 5, fig.align = 'center')
library(faraway)
data(bliss)
data(gala)
g <- gala[, -2]
```

# Hypothesis Testing

* Null model: intercept only model
* Saturated model: model with n obs and n parameters, each data point has its own parameter
* Proposed model: model with p parameters, model that you fit

## Deviance
Derived from the likelihood ratio statistic, deviance is defined as 

$$ D(y) = 2 \left( L(y \vert \hat{\theta}_s)) - L(y \vert \hat{\theta}_m)) \right) $$

where $$L$$ denotes the log likelihood, $$\hat{\theta}_m$$ denotes fitted values for the proposed model and $$\hat{\theta}_s$$ denotes fitted values for the saturated model.

The deviance is asymptotically distributed $$X^2_p$$.

## Pearson X2
The Pearson $$X^2$$ statistic is

$$X^2 = \Sigma_i \frac{(y_i - \hat{\mu}_i)^2}{var(\hat{\mu}_i)}$$

The Pearson $$X^2$$ is asymptotically distributed $$X^2_p$$.

## Goodness of Fit Test
Does the current model fit the data? The goodness of fit is a comparison of two models (the proposed model and the saturated model).

We can fit a glm in R and assess goodness of fit.
```{r}
mod1 <- glm(cbind(dead, alive) ~ conc, family = binomial, data = bliss)
summary(mod1)
```

* Null deviance: $$D_{sat} - D_{null}$$
* Residual deviance: $$D_{sat} - D_{model}$$

With these values we can conduct our hypothesis tests.
```{r}
# goodness of fit - using deviance
pchisq(mod1$deviance, mod1$df.residual, lower.tail = FALSE)
```
With a $$p.value = 0.94$$, we can conclude that there is no evidence of lack of fit. 

```{r}
# goodness of fit - using pearson X2
pchisq(sum(residuals(mod1, "pearson")^2), mod1$df.residual, lower.tail = FALSE)
```
We use the deviance residuals to compute the Pearson residuals $$X^2 = \Sigma r_{pearson}^2$$.

We see that the result is comparable to the deviance.

## Compare Two Models
Compare two nested models, which one is better?

```{r}
# test significance of conc by comparing to null mod1el
pchisq(mod1$null.deviance - mod1$deviance, mod1$df.null - mod1$df.residual, lower.tail = FALSE)
anova(mod1, test = "Chi")
```
Here we see that the concentration term is significant. 

We can also compare to more complex models.
```{r}
# compare to quadratic concentration term
mod2 <- glm(cbind(dead, alive) ~ conc + I(conc^2), family = binomial, bliss)
anova(mod1, mod2, test="Chi")
```
The results here indicate no need for a quadratic concentration term. 

# Diagnostics

## What Can Go Wrong

When we observe a deviance much larger than expected if the model was correct, we need to determine which aspects of the model specification is incorrect. 

* Observations are not independent
* Non linearity between covariates and link function: wrong structural form, explore different fits, transformations of predictors, etc
* Presence of outliers
* Sparse data: large samples/group sizes are needed for MLE asymptotic convergence
* Overdispersion: variance greater than assumed, which can arise when independent or identical assumptions are violated

Note that unlike in linear regression, our errors do not need to be normally distributed with constant variance.

## Residuals

* Response residuals: $$y - \hat{\mu}$$ has non-constant variance
* Pearson residuals: $$\frac{y - \hat{\mu}}{\sqrt{\hat{\phi}Var(\hat{\mu})}}$$, where $$\Sigma (r^p_i)^2 = X^2$$
* Deviance residuals: $$sign(y - \hat{\mu}) \sqrt{d}_i$$, where $$\Sigma (r^d_i)^2 = Deviance$$
* Jackknife residuals (studentized residuals): expensive to compute, but approximations are available

## Leverages
Since GLMs use the IRWLS algorithm, the leverage values are affected by the weights. The hat matrix is defined by
$$ H = W^{1/2}X(X'WX)^{-1}X'W^{1/2} $$

where $$W = diag(w)$$

The diagonal elements of $$H$$ contain the leverages $$h_{i}$$.

## Cook's Distance
The Cook statistics:
$$D_i = 
\frac{(\hat{\beta}_{(i)} - \hat{\beta})' (X'WX) (\hat{\beta}_{(i)} - \hat{\beta})}{p\hat{\phi}}$$

## DFBETAs
Similar to to the linear model, DFBETAs can examine the change in fit (coefficients) from omitting an observation.

## DIFDEV and DIFCHISQ
These statistics detect observations that heavily add to the lack of fit, when there is a large difference between the fitted and observed values.

DIFDEV measures the change in deviance if an individual observation is deleted.

DIFCHISQ measures the change in $$X^2$$ if an individual observation is deleted.

## How to Obtain Diagnostics in R

* Response residuals: `residuals(m, "response")`
* Pearson residuals: `residuals(m, "pearson")`
* Deviance residuals: `residuals(m)`
* Studentized residuals: `rstudent(m)`
* Leverage values: `influence(m)$hat`
* Cook statistics: `cooks.distance(m)`
* DFBETAs: `influence(m)$coef`

## Diagnostic Plots in R

### Residual vs Fitted Plots
```{r}
# fit model
mod <- glm(Species ~ ., family = poisson, data = g)
```

We can fit plots equivalent to the residuals vs fitted plots in linear regression. Since we use the deviance residuals (which are standardized), we should see constant variance. 
```{r, fig.width=10}
# deviance resids vs fitted response
g1 <- qplot(y = residuals(mod), x = predict(mod, type = "response")) + 
  xlab(expression(hat(mu))) + 
  ylab("Deviance Residuals")
# deviance residuals vs fitted link
g2 <- qplot(y = residuals(mod), x = predict(mod, type = "link")) + 
  xlab(expression(hat(eta))) + 
  ylab("Deviance Residuals")

# combine
grid.arrange(g1, g2, nrow = 1)
```
Two different scales for the fitted values. We see that using $$\hat{\eta}$$ is better than $$\hat{\mu}$$. Overall, we see that the residuals are evenly spaced across fitted values, and there are no violation of assumptions.

What should we do if we see violations?

* Nonlinear trend: consider transformation of covariates (generally better than changing the link function)
* Non-constant variance: change the model

When we plot using the response residuals, we will see variation patterns consistent with the response distribution.
```{r}
# response residuals vs fitted link
qplot(y = residuals(mod, "response"), x = predict(mod, type = "link")) + 
  xlab(expression(hat(eta))) + 
  ylab("Response Residuals")
```
Here we see a pattern of increasing variation consistent with the Poisson distribution. 

### Half-Normal Plot
We can plot the sorted absolute residuals to the quantiles of the half normal distribution to identify outliers. We look for points off the trend. (The examples used here are different from the model above).

```{r, echo = FALSE}
modpl <- glm(Species ~ log(Area) + log(Elevation) + log(Nearest) + log(Scruz+0.1) + log(Adjacent), family=poisson, gala)
```


```{r, fig.width = 10}
par(mfrow = c(1, 2))
# half-normal of studentized residuals
halfnorm(rstudent(modpl))
# half-norm of leverages
halfnorm(influence(modpl)$hat)
```
The half-norm plot of leverage seems to indicate that obs 25 may have high leverage. 

```{r}
# half-norm of cook's distance
c <- halfnorm(cooks.distance(modpl))
```
The half-norm plot indicates that obs 25 has a big Cook's statistic. It might be useful to investigate this observation in closer detail. 

### Added Variable Plots
Similar to regression, we can generate added variable plots. The interpretation is similar to linear models.

In R: `car::avPlots()`

# Overdispersion
Overdispersion occurs when the $$Var(Y_i)$$ is greater than the assumed $$Var(Y_i)$$ by the model. As a result of this, the model deviance could be inflated. To adjust for overdispersion, we can use quasilikelihood methods. 

## Estimate of Overdispersion Parameter
To account for overdispersion, we multiply the variance by a factor of $$\sigma^2$$ to obtain 

$$Var(Y_i)^* = \sigma^2 Var(Y_i)$$

If $$\sigma^2 = 1$$, we have the original model. If $$\sigma^2 > 1$$, then we have overdispersion. 

We can estimate $$\sigma^2$$ with a Fisher scoring algorithm used to fit the model.

When we apply overdispersion adjustment to modeling, the $$\hat{\beta}$$ coefficients is the same as the original model. The covariance matrix for $$\hat{\beta}$$ changes from $$Var(\hat{\beta}) = (X'WX)^{-1}$$ to $$Var(\hat{\beta}) = \sigma^2 (X'WX)^{-1}$$.

We can estimate $$\sigma^2$$ with 

$$\hat{\sigma}^2 = \frac{X^2}{n - p}$$

where $$X^2$$ is the Pearson goodness of fit statistic, $$n$$ is the number of observations, and $$p$$ is the number of parameters.

## Adjustment to Model

Adjustment to model statistics:

* $$SE(\hat{\beta})$$ adjusted to $$\sqrt{\hat{\sigma}^2} SE(\hat{\beta})$$
* Pearson residuals adjusted to $$r_i^{p*} = \frac{r_i^p}{\sqrt{\hat{\sigma}^2}}$$
* Pearson statistic adjusted to $$\frac{X^2}{\hat{\sigma}^2}$$
* Deviance residuals adjusted to $$r_i^{d*} = \frac{r_i^d}{\sqrt{\hat{\sigma}^2}}$$
* Deviance adjusted to $$\frac{deviance}{\hat{\sigma}^2}$$
* When comparing two nested models with dispersion, compare the result to a $$F$$ distribution

To fit a model with a dispersion parameter in R:

* Fit the model
```{r, eval = FALSE}
mod <- glm(cbind(damage, 6 - damage) ~ temp, data = orings, family = binomial)
```

* Compute the dispersion parameter
```{r, eval = FALSE}
o2 <- sum(residuals(mod, "pearson")) / (nrow(orings) - 1)
```

* Estimate the model with the dispersion parameter
```{r, eval = FALSE}
summary(mod, dispersion = o2, correlation = TRUE, symbolic.cor = TRUE)
```


# Solutions to Violation of Assumptions

* Non-linearity between covariates and link function: explore different fits, transformations of predictors, additional predictors, model selection, etc
* Presence of outliers: look into potential reasons (data entry error, scientific reason), fit model with and without the influential point and see if there is a difference, remove point
* Sparse data: find more data, this is needed for MLE and goodness of fit tests
* Overdispersion: adjust for dispersion using quasilikelihood
