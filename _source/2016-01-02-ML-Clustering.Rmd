---
layout: post
title: "Clustering Methods"
date: "January 2, 2016"
categories: ['statistics', 'machine learning']
---

* TOC
{:toc}

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(jn.general)
lib(data, viz)
knitr::opts_chunk$set(fig.width = 5, fig.height = 5, fig.align = 'center')
```

Clustering is an unsupervised learning technique. Unsupervised learning methods are employed when we only observe features and not the associated response variable. In these cases, we are not interested in prediction. The goal is to find an informative way of visualizing data or discover subgroups among the observations. Thus, it is more of an exploratory technique.

**Advantages:**

* Easier/cheaper to obtain unlabeled data than labeled data

**Disadvantages:**

* Without labeled data, one would never truely know if result is "correct"
* No implicit goal (unlike in supervised learning where the goal is prediction)

# Distance Metrics

There are a variety of distance metrics that we can use. All metrics have the following properties

* $$dist(x_i, x_j) \ge 0$$.
* $$dist(x_i, x_j) = 0$$ $$iff$$ $$x_i = x_j$$.
* $$dist(x_i, x_j) = dist(x_j, x_i)$$.
* $$dist(x_i, x_j) \le dist(x_i, x_k) + dist(x_k, x_j)$$.

Common distance metrics include

* Euclidean distance: $$d(x_i, x_k) = \sqrt{\sum^p_{j = 1} (x_{ij} - x_{kj})^2}$$
* Manhattan distance: $$d(x_i, x_k) = \sqrt{\sum^p_{j = 1} \vert x_{ij} - x_{kj} \vert}$$
* Correlation based distance: $$1 - r$$ (genearlly used in gene expression analyses)

Note that it is a good idea to standardize features prior to running algorithms. 

# K-Means Clustering
With K-means clustering, the goal is to partition observations into a pre-specified number of clusters. With K-means clustering

* All observations belong to a cluster
* Clusters are non-overlapping; each observation belong to only 1 cluster
* $$K$$ is chosen via cross-validation or a penalized clustering objective

## Optimization Equation
This is done by minimizing the within cluster variation.

$$min_{C_1...C_K} \sum^K_{k = 1} WCV(C_k)$$

where $$WCV(C_k)$$ is the amount by which the observations within a cluster differs.

The measurement of $$WCV(C_k)$$ is based on the distance metric defined by the user.

If we were to use Euclidean distance, we would have <br>
$$ WCV(C_k) = \sum_{i \in C_k} \sum^p_{j = 1} (x_{ij} - \bar{x}_{kj})^2$$

with the optimization problem being

$$ min_{C_1 ... C_K} \sum^K_{k = 1} \sum_{i \in C_k} \sum^p_{j = 1} (x_{ij} - \bar{x}_{kj})^2 $$

## Algorithm

1. Randomly assign a number, from $$1$$ to $$K$$, to each observation as the intial cluster assignments. Another option is to choose a $$K$$ random points to serve as the cluster centers and assign points to the nearest center. 
2. Iterate until cluster assignments consistent
  * For each of the $$K$$ clusters, compute the cluster centroid, the vector of the $$p$$ feature means for the observations in the $$k^{th}$$ cluster
  * Assign each observation to the cluster whose centroid is closest (based on the defined distance metric)

![K-means algorithm](http://jnguyen92.github.io/nhuyhoa/figure/images/cluster_kmeans_algorithm.png)

(Hastie, et.al)

This algorithm will find the local minimum for cluster assignments, but it is not guaranteed to find a global minimum. This is because we intially randomized the cluster assignments. 

To alleviate this problem, we can run the algorithm many times with random cluster assignments each time. In doing this, we can choose the replication that best miminizes our optimization problem and is (potentially) the global minimum. 

![K-means with random starts](http://jnguyen92.github.io/nhuyhoa/figure/images/cluster_kmeans_random_starts.png)

(Hastie, et.al)

## Example
We can use the iris data set to take a look at K-means clustering. Here is the true data.
```{r, echo = FALSE}
ggplot(data = iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) +
  geom_point(size = 1.5) +
  theme(legend.position = "none")
set.seed(1)
```

We will pretend that we don't know these groupings exist and run a K-means clustering algorithm. (Here we will set $$K = 3$$ but in real life we generally do not know how many groups there are).
```{r}
# generate the data
ir <- iris %>% dplyr::select(Petal.Length, Petal.Width)
# run K-means clustering
kmeans(ir, 3, nstart = 20)
```

Note that we run the algorithm 20 times to circumvent landing at a local minimum. 

```{r, echo = FALSE, fig.height=5, fig.width = 10}
set.seed(1)
true <- iris %>% 
  dplyr::select(-matches("Sepal")) %>% 
  mutate(Species = factor(Species), kind = "1 True Clusters")

comp <- ir %>% 
  mutate(Species = factor(kmeans(ir, 3, nstart = 20)$cluster), kind = "2 K-Means Clusters") %>% 
  rbind(true, .) %>% 
  mutate(Species = factor(as.numeric(Species)))

ggplot(data = comp, aes(x = Petal.Length, y = Petal.Width, color = Species)) +
  geom_point(size = 1.5) +
  theme(legend.position = "none") +
  facet_grid(~kind) +
  scale_colour_brewer(palette = "Set1")
```
Note that there is a little discrepancy, mostly at the boundaries of the two clusters. 

# K-Medoids Clustering

K-medoids clustering is similar to k-means clustering except that it uses a data point as a cluster center (rather than an average of all points). This quality makes this method more robust to outliers than the k-means method. 

These models can be fit in R with `cluster::pam()`,  `cluster::clara()`.

# Gaussian Mixture Models
Gaussian mixture model clustering is a method similar to K-means clustering. However, this technique uses a soft clustering method, where each cluster is represented by a distribution. In this case, we assume the data is generated by a mixture of Gaussians. Similar to K-means clustering, the value of $$K$$ is chosen via cross-validation or a penalized clustering objective. 

Recall the Gaussian distribution

$$f_j (\overrightarrow{x_i}) = \frac{1}{\sqrt{(2\pi)^p \vert \Sigma_j \vert}} exp\left( -\frac{1}{2} (\overrightarrow{x_i} - \overrightarrow{\mu_j})^T \Sigma_j^{-1} (\overrightarrow{x_i} - \overrightarrow{\mu_j}) \right) $$

## EM Algorithm
The EM algorithm sets the paramters of the Gaussians $$\Theta$$ to maximize the log likelihood of the data $$X$$.

------------------------------------|--------------------
$$\log(likelihood(X\vert \Theta))$$ | $$ = \log\prod^n_{i=1} P(\overrightarrow{x_i})$$
                                    | $$ = \log \prod^n_{i = 1} \sum^K_{k = 1} P_k(f_k(\overrightarrow{x_i}))$$
                                    | $$ = \sum^n_{i = 1} \log \sum^K_{k = 1} P_k(f_k(\overrightarrow{x_i}))$$

(We assume the covariance matrix is fixed and only adjust the means and weights) (Idk how to do the covariance matrix yet). 

The EM clustering algorithm involves

* Initialize parameters (means, cluster probabilities, etc)
* Loop until convergence using the E-step and the M-step

### Expectation Step
Let the hidden variable $$Z_{ij}$$ be $$1$$ if $$f_j$$ generated $$\overrightarrow{x_j}$$ and $$0$$ otherwise. The expectation of the hidden variable is 

$$h_{ij} = E(Z_{ij} = 1 \vert \overrightarrow{x_i}) = \frac{P_j(f_j(\overrightarrow{x_i}))}{\sum^K_{l = 1} P_l(f_l(\overrightarrow{x_i}))}$$

### Maximization Step
Given the expected values, we can re-estimate the means of the Gaussians and the cluster probabilities.

$$\overrightarrow{\mu_j} = \frac{\sum^n_{i = 1} h_{ij} \overrightarrow{x_i}}{\sum^n_{i = 1} h_{ij}} $$

$$P_j = \frac{\sum^n_{i = 1} h_{ij}}{n}$$

In this step, we may also re-estimate the covariance matrix if we were varying it (idk how yet).

# Hierarchical Clustering
With hierarchical clustering, the goal is to generate a tree-like visual representation of the observations (dendrogram) to view the clusterings obtained for each possible number of clusters. In this case, $$K$$ is not preset. 

## Algorithm
Hierarchical clustering uses a bottom-up clustering method to generate a dendeogram tree. This means that it grows a tree starting from the leaves. (Classification/regression trees use a top-down clustering method).

1. Begin with $$n$$ observations and a measure of all the $${n \choose 2}$$ pairwise dissimilarities. Add each cluster (individual observations) to the list of clusters to be joined
2. While number of clusters to be joined is greater than 1:
  * Examine all pairwise intercluster dissimilarities among the available clusters.
  * Fuse the pair of clusters that are most similar: remove pair of clusters and add its new fusion to list of clusters to be joined.
  * The dissimilarity b/n two clusters indicate the height of dendrogram at which fusion is placed.
  * Compute new pairwise intercluster dissimilarities among the $$i - 1$$ remaining clusters.

![hierarchical clustering algorithm](http://jnguyen92.github.io/nhuyhoa/figure/images/cluster_hierarchical_algorithm.png)

## Dissimilarity Metrics

* **Complete:**

$$dist(c_u, c_v) = min[dist(a, b) \vert a \in c_u, b \in c_v] $$

* **Single:**

$$dist(c_u, c_v) = max[dist(a, b) \vert a \in c_u, b \in c_v] $$

* **Average:**

$$dist(c_u, c_v) = avg[dist(a, b) \vert a \in c_u, b \in c_v] $$

![complete vs single linkages](http://jnguyen92.github.io/nhuyhoa/figure/images/cluster_complete_vs_single_link.png)

The type of linkage will affect the clustering results.

* Single linkage has the tendency to form long chains
* Complete linkage are sensitive to outliers
* Average linkage try to find a compromise between the two options

![Comparisons of Linkages](http://jnguyen92.github.io/nhuyhoa/figure/images/cluster_hierarchical_link_disadvantages.png)

(Hastie, et.al)

## Interpretting a Dendrogram

* Height of the fusion (vertical axis) indicates how different the observations (lower is more similar)
* Horizontal axis has no meaning

![dendrogram](http://jnguyen92.github.io/nhuyhoa/figure/images/cluster_hierarchical_dendrogram.png)

* Draw a horizontal line across a dendrogram to identify the clusters

![dendrogram cutting for clusters](http://jnguyen92.github.io/nhuyhoa/figure/images/cluster_hierarchical_cutting.png)

## Example
In this example, we will use a subset of the iris data.
```{r, echo = FALSE}
ir <- iris %>% dplyr::select(Petal.Length, Petal.Width)
rownames(ir) <- paste0(1:150, ": ", iris$Species)
set.seed(1)
```

```{r, warning = FALSE}
# run hierarchical cluster on a subset of iris data
ir <- sample_frac(ir, 0.25)
hc <- hclust(dist(ir), method = "average")

# plot data
ggdendro::ggdendrogram(hc)
```

If we cut the dendrogram into 3 groups, we have pretty solid cluser for the $$setosa$$ group (homogeneous and small cluster size). The other two groups are have some misclassifications, potentially due to the hazy boundry line between the two groups. 

```{r, warning = FALSE}
hc2 <- as.dendrogram(hc)

# color the labels
plot(dendextend::color_labels(hc2, 3))

# color the brances
plot(dendextend::color_branches(hc2, 3))
```

# DBSCAN
The DBSCAN algorithm is a density based clustering algorithm. It groups objects into one cluster if they are connected to one another by a densely populated area.

The tuning parameters of this method includes

* reachability distance which defines the size of the neighborhood
* minimum number of points 

Data point classifications:

* **Core point**: a single data point $$p$$ is a core point if there are at least the minimimum specified number of points in its neighborhood. These points in the neighborhood are directly reachable
* **Reachable points**: a data point $$q$$ is reachable from $$p$$ if there is a path from $$p$$ to $$q$$ where each preceding point is directly reachable from the suceding point
* **Outliers**: all points not reachable from any other point

A cluster satisfies the following properties:

* All points within the cluster are mutually density connected
* If a point is density reachable from any point of the cluster, it is a part of the cluster

This method has several advantages:

* Insensitive to noise and outliers
* Can discover clusters of various shapes and sizes

In R, DBSCAN can be implemented with `fpc::dbscan()`

# Evaluation of Clusters

**Internal Validation:**

* Sensitivity analysis: resample or add noise. If the clusters are stable, you would not expect to see major changes in clusters from replications
* Assess how well clustering optimizes the intracluster similarity and intercluster dissimilarity

The Silhouette index: <br>
$$\frac{1}{K} \sum^K_{j = 1} \frac{1}{\vert C_j \vert} \sum_{x_i \in C_j} \frac{b(x_i) - a(x_i)}{max[b(x_i), a(x_i)]}$$

where $$b(x_i)$$ is the average distance of $$x_i$$ to instances in the next closest cluster and $$a(x_i)$$ is the average distance of $$x_i$$ is the average distance of $$x_i$$ to other instances in the same cluster. 

The more positive the SI, the better the clusters. 

**External Validation:**

* Use external/domain knowledge to assess clusters
* Cross-classify features from experiment with features from literature into a contingency table; can perform a fischer's exact test or chi-square test to see if there is a significant association from experiment and literature
  
# Clustering in Bioinformatics

Some topics and tips:

* Visualize clusters by plotting response vs treatment for different clusters to examine whether certain clusters identify certain treatment effects
* Cluster aggregated data (by treatment) rather than individual samples to reduce noise
* Centering and scaling may make it hard to distinguish those with a systematic (trt) effect from those without; try just centering instead
