---
layout: post
title: "GLM: Multinomial Regression"
date: "November 26, 2015"
categories: statistics
---

* TOC
{:toc}

```{r, echo = FALSE, message = FALSE}
library(jn.general)
lib(data, viz)
knitr::opts_chunk$set(fig.width = 5, fig.height = 5, fig.align = 'center', message = FALSE)
gator <- fread("gator.csv")
setnames(gator, c("length", "choice"))
library(faraway)
cheese <- fread("cheese.dat")
setnames(cheese, c("cheese", "response", "N"))
cheese$response <- factor(cheese$response, ordered = TRUE)
```

# Nomial Regression
Let {$$\pi_1, ... \pi_J$$} denote the response probabilities satisfying $$\Sigma_j\pi_j = 1$$. With $$n$$ independent observations the probability distribution for the number of outcomes of the $$J$$ types is the multinomial.
$$P(Y_{i1} = y_{i1}, ..., Y_{iJ} = y_{iJ}) = \frac{n_i}{y_{i1}! ... y_{iJ}!} \pi^{y_{i1}}_{i1}...\pi^{y_{iJ}}_{iJ}$$

where $$J$$ denotes the number of categories for $$Y$$.  Nomial regression is just an extension of binomial logistic regression. 

## Interpreting Models and Coefficients
When the last category ($$J$$) is the baseline, the baseline-cateogry logits with predictor $$x$$ are

$$ log(\pi_i/\pi_1) = \beta_{i0} + \beta_{i1} x_1 + ... + \beta_{ip} x_p$$

where $$i = 2,..., J$$. There are $$J-1$$ equations, with separate parameters for each. (When $$J = 2$$, model simplifies to the ordinary logistic regression for binary responses).  

Another way to look at this is via the link function

$$\eta_{ij} = x'_i \beta_j = log \left( \frac{\pi_{ij}}{\pi_{i1}} \right) $$

To compare two categories where neither is the baseline, we can do some simple rearrangements.

$$ log\left( \frac{\pi_a}{\pi_b} \right) = log\left( \frac{\pi_a/\pi_1}{\pi_b/\pi_1} \right) = log\left( \frac{\pi_a}{\pi_1} \right) - log\left( \frac{\pi_b}{\pi_1} \right)$$
$$ = (\alpha_a + \beta_a * x) - (\alpha_b + \beta_b * x) $$
$$ = (\alpha_a - \alpha_b) + (\beta_a - \beta_b)*x $$

where $$(\alpha_a - \alpha_b)$$ is the intercept parameter with slope parameter $$(\beta_a - \beta_b)$$ for the new comparison. 

Interpretation of coefficients is identical to logistic regression case. For example, the equation
$$log\left( \frac{\pi_a}{\pi_b} \right) = \alpha + \beta_1 x_1 + ... + \beta_p + x_p$$

Holding all other covariates constant, a unit increase in $$x_i$$ will lead to an increase in odds of falling into category $$a$$ over category $$b$$ by a factor of $$exp(\beta_i)$$.

## Estimating Response Probabilities
Similar to logistic regression to estimate response probabilities, we have

$$ \hat{\pi}_i = \frac{exp(\eta_{ij})}{1 + \Sigma^J_{j = 2} exp(\eta_{ij})}$$

where $$\Sigma_i \pi_i = 1$$ and $$\eta_{i1} = 0$$. 

We can also do this in R:

* Response probabilities: `predict(mod, newdata, type = "probs")`
* Class: `predict(mod, newdata, type = "class")`

## Example
With this example, we examine how aligator length can be used to predict the gator's food choice. 
```{r}
ggplot(data = gator, aes(x = choice, y = length)) + 
  geom_boxplot() +
  xlab("Food Choice") + ylab("Length") + 
  ggtitle("Gator Length vs Food Choice")
```
From the boxplot, we see a trend in food choice and gator length.

So let's fit a multinomial model.
```{r, message = FALSE}
# package to fit multinomial models
library(nnet)

# fit model
mod1 <- multinom(choice ~ length, data = gator)
summary(mod1)
```

From this, we get the multinomial equations 
$$ log(\pi_O/\pi_F) = -1.618 + 0.1101x $$
$$ log(\pi_I/\pi_F) = 4.089 - 2.3553x $$

Thus we can use both these equations to calculate  
$$log(\pi_O/\pi_I) = log(\pi_O/\pi_F) - log(\pi_I/\pi_F) $$
$$ = (-1.618 + 0.1101x) - (4.089 - 2.3553x) $$
$$ = -5.707 + 2.4654x$$

For the alligators of length $$x + 1$$ meters, the estimated odds that primary food type is "invertebrate" rather than "fish" equal $$exp(-2.3553) = 0.0945$$ times the estimated odds at length $$x$$ meters.  

Note: to switch the odds (go from $$\pi_1/\pi_2$$ to $$\pi_2/\pi_1$$) just switch the signs of the equation on the RHS.  

Original equation:
$$ log(\pi_1/\pi_2) = \alpha_1 + \beta_1x $$ 

Flipped equation:
$$ log(\pi_2/\pi_1) = log( (\pi_1/\pi_2)^{-1} ) = -log(\pi_1/\pi_2) = -\alpha_1 + -\beta_1x $$ 


We can also compute the response probabilities using the equation above. 
$$ \hat{\pi}_O = \frac{exp(-1.618 + 0.1101x)}{1 + exp(-1.618 + 0.1101x) + exp(4.089 - 2.3553x)} $$
$$ \hat{\pi}_I = \frac{exp(4.089 - 2.3553x)}{1 + exp(-1.618 + 0.1101x) + exp(4.089 - 2.3553x)} $$
$$ \hat{\pi}_F = \frac{1}{1 + exp(-1.618 + 0.1101x) + exp(4.089 - 2.3553x)} $$

These probabilities can be used to plot the probabilities of various food preferences in gators across lengths.

```{r, echo = FALSE, fig.width = 6}
# fitted probabilities and gator lengths
fits <- fitted(mod1) %>% data.frame 
fits$length <- gator$length

# melt data for plotting
plot_data <- melt(fits, "length")
setnames(plot_data, c("variable", "value"), c("choice", "probability"))

# find indices of choices
choice_F <- gator$choice == "F"
choice_I <- gator$choice == "I"
choice_O <- gator$choice == "O"

# plot data, separate lines for choices
ggplot(data = plot_data, aes(x = length, y = probability, color = choice)) + 
  geom_line(linetype = 2) + ylim(c(0, 1)) +
  ggtitle("Response Probability of Food Choice by Gator Length") + 
  # plot empirical data
  annotate("text", x = gator$length[choice_F], y = fits$F[choice_F], label = "F", color = "red") +
  annotate("text", x = gator$length[choice_I], y = fits$I[choice_I], label = "I", color = "green") +
  annotate("text", x = gator$length[choice_O], y = fits$O[choice_O], label = "O", color = "blue")
```


# Ordinal Regression
With ordinal categorical variables, we will discuss the proportional odds logistic regression model, which can account for ordering using cumulative probabilities.

A cumulative probability for $$Y$$ is the probability that $$Y$$ falls at or below a particular point. For outcome category $$j$$, the cumulative probability is 

$$P(Y \le j) = p_1 + ... + p_j $$ 

where $$j = 1, ..., J - 1$$. 

The cumulative probabilities reflect the ordering, with 

$$ P(Y \le 1) \le P(Y \le 2) \le ... \le P(Y \le J) = 1$$

The logits of cumulative probabilities are

$$ logit \big[ P(Y \le j) \big] = log \left( \frac{P(Y \le j)}{1 - P(Y \le j)} \right) = log \left( \frac{p_1 + ... p_j}{p_{j + 1} + ... + p_J} \right)$$

## Interpreting Models and Coefficients
For ordinal categorical variables, we have the model

$$ logit \big[ P(Y \le j ) \big] = \theta_j - x' \beta $$

where $$j = 1,...,J-1$$ and where $$\beta$$ describes the effect of $$x$$ on the log odds of response in category j or below. This model assumes that the effect of $$x$$ is identical for all $$J-1$$ cumulative logits. In other words, we would have parallel cumulative probability lines. 

Because the covariate terms are subtracted, the $$\beta$$s are interpreted with respect to the $$log \left( \frac{P(Y > j)}{P(Y \le j)} \right)$$ instead of $$log \left( \frac{P(Y \le j)}{P(Y > j)} \right)$$.

For example, assume we have 3 ordered categories and 2 predictors. Thus, we would have

$$ logit \big[ P(Y \le 1 ) \big] = \alpha_1 - \beta_1 x_1 - \beta_2 x_2$$
$$ logit \big[ P(Y \le 2 ) \big] = \alpha_2 - \beta_1 x_1 - \beta_2 x_2$$
$$ logit \big[ P(Y \le 3 ) \big] = 1 $$

The intercept $$\alpha_j$$ is the log-odds of falling into or below category $$j$$ for $$x_1 = x_2 = 0$$. 

The slope parameter $$\beta_k$$ can be interpreted as so: holding all other covariates constant, a unit increase in $$x_k$$ increases the odds of above any category by a factor of $$exp(\beta_k)$$. Another way to interpret this is holding all other covariates constant, a unit increase in $$x_k$$ increases the odds of falling into or below any category by a factor of $$exp(-\beta_k)$$

We can also obtain $$P(Y = j)$$ with $$P(Y = j) = P(Y \le j) - P(Y \le j - 1)$$.

When $$\beta > 0$$, we have cumulative probability plots similar to the one below. Note the parallel curves.

![cumulative probabilities in proportional odds model][cum_prob_prop_odds_model]

When $$\beta < 0$$, the curves in Figure 6.2 descend rather than ascend.

## Estimating Response Probabilities
To obtain the response probabilities, we have

$$ P(Y \le j) = \frac{exp(\theta - \beta' x_i)}{1 + exp(\theta_j - \beta' x_i)}$$ 

## Example

```{r}
# package to fit model
library(MASS)

# fit model
mod2 <- polr(response ~ cheese, weights = N, data = cheese)
summary(mod2)
```
One thing to note is that R fits the model $$log \left( \frac{P(Y \le j)}{P(Y > j)} \right) = \theta_j - x' \beta$$. This affects how we interpret the coefficients on the $$\beta$$s.

For the cheese variable, we have the baseline dummy as $$A$$. The coefficient for $$cheeseB = -3.352$$. The responses are ordered 1 - 9, with bigger numbers indicating better responses. Thus we can interpret the $$cheeseB$$ coefficient as so:
$$\frac{odds.B.better}{odds.A.better} = exp(\beta_{cheeseB}) = exp(âˆ’3.352) = 0.035$$ 

The odds that cheese B is ranked "better" is $$0.035$$ times the odds that cheese A is ranked "better". Therefore cheese A is preferred over cheese B. The t-value is relatively big in magnitude so we can conclude that this difference in preference is significant. Also note that this ranking holds over all responses, due to the proportional odds assumption. That is, cheese A is preferred over cheese B regardless of whether "better" is a response cutoff of 3 or 7 (or any other value). We can do similar comparisons for other cheese types. 

We can use the results to generate our individual equations.
$$ log \left( \frac{P(Y \le 1)}{P(Y > 1)} \right) = -5.4674 - - 3.352X_1 - - 1.710X_2 - 1.613X_3) $$
$$ log \left( \frac{P(Y \le 2)}{P(Y > 2)} \right) = -4.4122 - - 3.352X_1 - - 1.710X_2 - 1.613X_3) $$
$$...$$
$$ log \left( \frac{P(Y \le 8)}{P(Y > 8)} \right) = 3.1058 - - 3.352X_1 - - 1.710X_2 - 1.613X_3) $$

Note that we add the intercept specified by the model, but subtract the $$\beta$$ coefficients.

We can also calculate individual probabilities at the baseline cheese (A):

For $$response = 1$$
$$P(Y = 1) = P(Y \le 1) = ilogit(-5.4674) = 0.0042$$

For $$response = 2$$
$$P(Y = 2) = P(Y \le 2) - P(Y \le 1) = ilogit(-4.4122) - ilogit(-5.4674) = 0.00778$$

For $$response = 3$$
$$P(Y = 3) = P(Y \le 3) - P(Y \le 2) = ilogit(-3.3126) - ilogit(-4.4122) = 0.023$$

For $$response = 9$$
$$P(Y = 9) = 1 - P(Y \le 8) = 1 - ilogit(3.1058) = 0.0429$$

# Model Testing and Diagnostics
Similar to other glms, we can use deviance and the likelihood ratio test to compare two models. 

[cum_prob_prop_odds_model]: /figure/images/cum_probs_prop_odds_model.png
