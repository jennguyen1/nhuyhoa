---
layout: post
title: "Multiple Comparisons"
date: "November 10, 2015"
categories: statistics
---

* TOC
{:toc}

```{r global_opts, echo = FALSE}
library(jn.general)
lib(data, viz)
```

# Why It's Needed
When we want to conduct multiple pairwise tests, we need account for the number of tests. Assume that each pairwise test has a type I error rate of $$\alpha = 0.05$$. 

If we conduct $$p$$ number of tests, the probability that we don't make an error on any of those tests is $$(1 - \alpha)^p$$. 
```{r, echo = FALSE}
n <- c("1", "2", "...", "p")
p <- c("(1 - a)^1", "(1 - a)^2", "...", "(1 - a)^p")
data.frame(num_tests = n, prob_no_error = p) %>% kable
```

From this we can easily compute the probability of making an error if we conducted $$p$$ tests. 
```{r, echo = FALSE}
n <- c("1", "2", "...", "p")
p <- c("1 - (1 - a)^1", "1 - (1 - a)^2", "...", "1 - (1 - a)^p")
data.frame(num_tests = n, prob_error = p) %>% kable
```

```{r, echo = FALSE}
# plot n vs probability of error
n <- 1:100
p <- (1 - (1 - 0.05)^n)
qplot(x = n, y = p, size = I(1.5), geom = "line", main = "Type 1 Error Rate vs. Number of Tests") +
  xlab("Number of Tests") +
  ylab("Probability of Type 1 Error") 
```

Our error rates rapidly increases. The type 1 error rate is approximately $$50$$% when we conduct $$13$$ tests. Thus we have to find a way to correct for these multiple tests to ensure that our Type 1 error rate remains low.

# How to Do It