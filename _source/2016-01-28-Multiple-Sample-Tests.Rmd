---
layout: post
title: "Multiple Sample Tests"
date: "January 28, 2016"
categories: ['statistics', 'experimental design']
---

* TOC
{:toc}

```{r, echo = FALSE}
library(jn.general)
lib(data, viz)
knitr::opts_chunk$set(fig.width = 5, fig.height = 5, fig.align = 'center')
```

# ANOVA
In R, ANOVA can be fit with the `lm()` and `anova()` or `aov()` commands.

**Assumptions**

Regardless of the type, ANOVAs have the same assumptions:

1. Independence: within and across treatments
2. Normality: $$Y_{ij}$$ ~ $$N(\mu_i, \sigma^2_i)$$
3. Equal Variance: $$\sigma^2_1 = ... = \sigma^2_k$$

## Single Factor ANOVA 

**Model Formulations**

Consider a design with $$k$$ groups where there are $$n_i$$ observations on the $$i^{th}$$ treatment. Let $$y_{ij}$$ denote the $$j^{th}$$ observation on the $$i^{th}$$ treatment. 

$$Y_{ij} = \mu_i + \epsilon_{ij}$$ 

where $$\epsilon_{ij}$$ ~ iid$$N(0, \sigma^2_{\epsilon})$$

$$Y_{ij} = \mu + \alpha_i + \epsilon_{ij}$$ 

where $$\epsilon_{ij}$$ ~ iid$$N(0, \sigma^2_{\epsilon})$$. We assume $$\sum \alpha_i = 0$$ if $$H_0$$ is true.

**ANOVA Table**

We have the following terms

* treatment sum: $$y_{i.} = \sum^{n_i}_{j = 1} y_{ij}$$
* treatment mean: $$\bar{y}_{i.} = y_{i.}/n_i$$
* overall sum: $$y_{..} = \sum^k_{i = 1} \sum^{n_i}_{j = 1} y_{ij}$$
* overall mean: $$\bar{y}_{..} = y_{..} / N$$

Source| Sum of Squares | Degrees of Freedom | Mean Square | F 
------|----------------|--------------------|-------------|---------
Trt   | $$\sum^k_{i = 1} n_i(\bar{y}_{i.} - \bar{y}_{..})^2$$ | $$k-1$$ | $$\frac{SSTrt}{dfTrt}$$ | $$\frac{MSTrt}{MSE}$$ 
Error | $$\sum^k_{i = 1} \sum^{n_i}_{j = 1} (y_{ij} - \bar{y}_{i.})^2 = \sum^k_{i = 1} (n_i - 1) s_i^2$$ | $$N-k$$ | $$\frac{SSE}{dfE}$$ |
Total | $$\sum^k_{i = 1} \sum^{n_i}_{j = 1} (y_{ij} - \bar{y}_{..})^2 = \sum_{all.obs} (y_{ij} - \bar{y}_{..})$$ | $$N-1$$ | | 

<p></p>
 
where $$F$$ ~ $$F_{dfTrt, dfErr}$$. When $$F$$ is large, we say that the group effect is large so we reject the null hypothesis that the groups are the same.

We can derive this distribution from [probability theory][stat_theory_link]{:target = "_blank"}. The value $$\frac{df * MS}{\sigma^2}$$ is distributed $$X^2$$. So 

$$F = \frac{ \frac{df * MS}{\sigma^2} / df }{ \frac{dfErr * MSErr}{\sigma^2} / dfErr } = \frac{ MSTrt }{ MSErr }$$ 

is distributed $$F_{df, dfErr}$$. 

Another way to look at this is to consider if $$H_0$$ is true

* .$$\sum_i \alpha_i = 0$$
* .$$E[MSTrt] = \sigma^2_{\epsilon} + \frac{n}{k - 1} \sum^k_i \alpha_i^2$$
* .$$E[MSE] = \sigma^2_{\epsilon}$$

Then 

$$\frac{E[MSTrt]}{E[MSE]} = 1 + \frac{1}{k - 1} \frac{n \sum^k_i \alpha^2_i}{\sigma^2_{\epsilon}}$$ 

Note that if $$\sum \alpha_i \approx 0$$, then this ratio $$\approx 1$$. The power of the $$F$$ test is a monotone function of $$\frac{n \sum^k_i \alpha^2_i}{\sigma^2_{\epsilon}}$$.

## Two Factor ANOVA

**Model Formulations**

$$Y_{ijl} = \mu + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijl}$$

where

* $$i = 1, ..., a$$ denotes the levels of factor A
* $$j = 1, ..., b$$ denotes the levels of factor B
* $$l = 1, ..., n$$ denotes replicates of each factor combination
* $$\epsilon_{ijl}$$ ~ $$N(0, \sigma^2_{\epsilon})$$ represents the plot error


**ANOVA Table**

Source| Sum of Squares | Degrees of Freedom | Mean Square | F 
------|----------------|--------------------|-------------|---------
A     | $$bn\sum^a_{i = 1} (\bar{y}_{i..} - \bar{y}_{...})^2$$ | $$a-1$$ | $$\frac{SSA}{dfA}$$ | $$\frac{MSA}{MSE}$$ 
B     | $$an\sum^b_{j = 1} (\bar{y}_{.j.} - \bar{y}_{...})^2$$ | $$b-1$$ | $$\frac{SSB}{dfB}$$ | $$\frac{MSB}{MSE}$$ 
AB    | $$n \sum^a_{i = 1} \sum^b_{j = 1} (\bar{y}_{ij.} - \bar{y}_{i..} - \bar{y}_{.j.} + \bar{y}_{...})^2$$ | $$(a-1)(b-1)$$ | $$\frac{SSAB}{dfAB}$$ | $$\frac{MSAB}{MSE}$$ 
Error | $$\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{l = 1} (y_{ijl} - \bar{y}_{ij.})^2$$ | $$ab(n - 1)$$ | $$\frac{SSE}{dfE}$$ | | 
Total | $$\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{l = 1} (y_{ijl} - \bar{y}_{...})^2$$ | $$abn - 1$$ | | 

<p></p>

Note that we have three separate $$F$$ tests (similar to the single factor case) to assess the $$A_{main}$$, $$B_{main}$$, and $$AB_{int}$$ effects. 

Assume $$H_0$$ is true so that

* $$\sum_i \alpha_i = 0$$, $$\sum_j \beta_j = 0$$ and $$\sum_i (\alpha \beta)_{ij} = \sum_j (\alpha \beta)_{ij} = 0$$
* .$$E[MSA] = \sigma^2_{\epsilon} + \frac{bn}{a - 1} \sum \alpha^2_i$$
* .$$E[MSB] = \sigma^2_{\epsilon} + \frac{an}{b - 1} \sum \beta^2_j$$
* .$$E[MSAB] = \sigma^2_{\epsilon} + \frac{n}{(a - 1)(b - 1)} \sum (\alpha \beta)^2_{ij}$$
* .$$E[MSE] = \sigma^2_{\epsilon}$$

We can test the main effects and interaction by comparing them to $$MSE$$ with an $$F$$ test.

**Contrasts**

Let $$\bar{ab}$$ denote the mean of the high-high group, $$\bar{a}$$ denote the mean of high-low group, $$\bar{b}$$ denote the mean of the low-high group, and $$\bar{1}$$ denote the mean of the low-low group. We have

* .$$A_{main} = \frac{1}{2} [(\bar{ab} - \bar{b}) + (\bar{a} - \bar{1})]$$
* .$$B_{main} = \frac{1}{2} [(\bar{ab} - \bar{a}) + (\bar{b} - \bar{1})]$$
* .$$AB_{int} = \frac{1}{2} [(\bar{ab} - \bar{b}) - (\bar{a} - \bar{1})]$$

We can assess these effects by generating an interaction plot. By assessing the trends and parallelism of lines, we can determine whether there may be evidence of an interaction effect. We can also test these effects with a contrast test.

## Randomized Complete Block Design ANOVA

Block designs are very similar to regular ANOVAs. Blocking is similar to a paired analysis in t-tests. We expect block to block variability. The observations in the same block share a block effect and are less variable than observations in other blocks. By accounting for the block variability, we decrease the unaccounted variability and make the test for treatment more powerful. 

A single block should be homogeneous. To ensure this homogeneity, we want to make sure blocks are small enough that

* all treatments are contained with a block but
* there are no duplicated treatments within a block

**Model Formulations**

$$Y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}$$

where

* $$i = 1, ..., k$$ denotes the levels of treatment
* $$j = 1, ..., b$$ denotes the levels of blocks
* $$\epsilon_{ij}$$ ~ $$N(0, \sigma^2_{\epsilon})$$ represents the plot error

**ANOVA Table**

Source| Sum of Squares | Degrees of Freedom | Mean Square | F 
------|----------------|--------------------|-------------|---------
Blokcs| $$k\sum^b_{j = 1} (\bar{y}_{.j} - \bar{y}_{..})^2$$ | $$b-1$$ | $$\frac{SSB}{dfB}$$ | $$\frac{MSB}{MSE}$$ 
Trt   | $$b\sum^k_{i = 1} (\bar{y}_{i..} - \bar{y}_{...})^2$$ | $$k-1$$ | $$\frac{SSA}{dfA}$$ | $$\frac{MSA}{MSE}$$ 
Error | $$\sum_{ij} (\bar{y}_{ij} - \bar{y}_{i.} - \bar{y}_{.j} + \bar{y}_{..})^2$$ | $$(k-1)(b-1)$$ | $$\frac{SSE}{dfe}$$ | 
Total | $$\sum_{ij} (y_{ij} - \bar{y}_{..})^2$$ | $$kb - 1$$ | | 

<p></p>

We can assess the effect of treatment with an $$F$$ test on $$k - 1$$ and $$(k - 1)(b - 1)$$ df. 

Note that we can ignore the $$F$$ test for the effect of blocks. The blocks were used in order to account for variability. A significant test would indicate that we were right to block. If we did not find a significant block effect, we should not pool the block with error and analyze the data as a completely randomized design! We need to analyze the data as we have designed it, otherwise we would bias our tests. 

We notice several things from this table. One is that this table is reminiscent of the ANOVA table for two-factor ANOVA where $$n = 1$$. Another similarity is that the SS for error is similar to the SS for the interaction term in the two-factor case. This is because we assume an additive model and so the error that is normally given to the interaction is transferred to the error.

Thus RCBD with one factor is the same as a completely randomized two-factor ANOVA where $$n = 1$$.

## Three Factor ANOVA

**Model Formulations**

$$Y_{ijkl} = \mu + \alpha_i + \beta_j + \gamma_k + (\alpha \beta)_{ij} + (\alpha \gamma)_{ik} + (\beta \gamma)_{jk} \epsilon_{ijkl}$$

where

* $$i = 1, ..., a$$ denotes the levels of factor A
* $$j = 1, ..., b$$ denotes the levels of factor B
* $$k = 1, ..., c$$ denotes the levels of factor C
* $$l = 1, ..., n$$ denotes replicates of each factor combination
* $$\epsilon_{ijkl}$$ ~ $$N(0, \sigma^2_{\epsilon})$$ represents the plot error

**ANOVA Table**

Source| Sum of Squares | Degrees of Freedom | Mean Square | F 
------|----------------|--------------------|-------------|---------
A     | $$bcn\sum^a_{i = 1} (\bar{y}_{i...} - \bar{y}_{....})^2$$ | $$a-1$$ | $$\frac{SSA}{dfA}$$ | $$\frac{MSA}{MSE}$$ 
B     | $$acn\sum^b_{j = 1} (\bar{y}_{.j..} - \bar{y}_{....})^2$$ | $$b-1$$ | $$\frac{SSB}{dfB}$$ | $$\frac{MSB}{MSE}$$ 
C     | $$abn\sum^b_{k = 1} (\bar{y}_{..k.} - \bar{y}_{....})^2$$ | $$c-1$$ | $$\frac{SSC}{dfC}$$ | $$\frac{MSC}{MSE}$$ 
AB    | $$cn \sum^a_{i = 1} \sum^b_{j = 1} (\bar{y}_{ij..} - \bar{y}_{i...} - \bar{y}_{.j..} + \bar{y}_{....})^2$$ | $$(a-1)(b-1)$$ | $$\frac{SSAB}{dfAB}$$ | $$\frac{MSAB}{MSE}$$ 
AC    | $$bn \sum^a_{i = 1} \sum^c_{k = 1} (\bar{y}_{i.k.} - \bar{y}_{i...} - \bar{y}_{..k.} + \bar{y}_{....})^2$$ | $$(a-1)(c-1)$$ | $$\frac{SSAC}{dfAC}$$ | $$\frac{MSAC}{MSE}$$ 
BC    | $$an \sum^b_{j = 1} \sum^c_{k = 1} (\bar{y}_{.jk.} - \bar{y}_{.j..} - \bar{y}_{..k.} + \bar{y}_{....})^2$$ | $$(b-1)(c-1)$$ | $$\frac{SSBC}{dfBC}$$ | $$\frac{MSBC}{MSE}$$ 
ABC   | $$n \sum_{ijkl} (y_{ijk.} - \bar{y}_{ij..} - \bar{y}_{i.k.} - \bar{y}_{.jk.} + \bar{y}_{i...} + \bar{y}_{.j..} + \bar{y}_{..k.} - \bar{y}_{....})^2$$ | $$(a-1)(b-1)(c-1)$$ | $$\frac{SSABC}{dfABC}$$ | $$\frac{MSABC}{MSE}$$
Error | $$\sum_{ijkl} (y_{ijkl} - \bar{y}_{ijk.})^2$$ | $$abc(n - 1)$$ | $$\frac{SSE}{dfE}$$ | | 
Total | $$\sum_{ijkl} (y_{ijk.} - \bar{y}_{...})^2$$ | $$abcn - 1$$ | | 

<p></p>

We need to consider the hierarchy principal when we consider these effects. If a higher order term is important, that means that a lower order term is important as well (and as in linear regression, should not be removed from the model). 

**Contrasts**

For contrasts, we have

* .$$A_{main} = \frac{1}{4} [(\bar{abc} - \bar{bc}) + (\bar{ab} - \bar{b}) + (\bar{ac} - \bar{c}) + (\bar{a} - \bar{1})]$$
* .$$B_{main} = \frac{1}{4} [(\bar{abc} - \bar{ac}) + (\bar{ab} - \bar{a}) + (\bar{bc} - \bar{c}) + (\bar{b} - \bar{1})]$$
* .$$C_{main} = \frac{1}{4} [(\bar{abc} - \bar{ab}) + (\bar{ac} - \bar{a}) + (\bar{bc} - \bar{b}) + (\bar{c} - \bar{1})]$$

* .$$AB_{int} = \frac{1}{4} [(\bar{abc} - \bar{bc}) - (\bar{ac} - \bar{c}) + (\bar{ab} - \bar{b}) - (\bar{a} - \bar{1})]$$
* .$$AC_{int} = \frac{1}{4} [(\bar{abc} - \bar{bc}) - (\bar{ab} - \bar{b}) + (\bar{ac} - \bar{c}) - (\bar{a} - \bar{1})]$$
* .$$BC_{int} = \frac{1}{4} [(\bar{abc} - \bar{ac}) - (\bar{ab} - \bar{a}) + (\bar{bc} - \bar{c}) - (\bar{b} - \bar{1})]$$

* .$$ABC_{int} = \frac{1}{4} [(\bar{abc} - \bar{bc}) - (\bar{ac} - \bar{c}) - (\bar{ab} - \bar{b}) - (\bar{a} - \bar{1})]$$

We can assess these effects by generating an interaction plot. We can also test these effects with a contrast test.

# Pairwise Comparisons and Contrasts

After rejecting a test that the means of the groups are not equal, we want to know exactly which ones are different. 

A contrast is a linear function of the group means. It can be used to compare two means or any set of groups of groups.

Procedure:

* An arbitrary contrast $$C = \Sigma^r_{i = 1} c_i \mu_i$$ where $$\Sigma^r_{i = 1} c_i = 0$$. There can be an infinite number of contrasts.
* $$C$$ is estimated with $$\hat{C} = \Sigma^r_{i = 1} c_i \bar{Y}_i$$ 
* The variance of the estimate $$\hat{C}$$ is $$s_{\hat{C}}^2 = \sigma^2_e \Sigma^r_{i = 1} \frac{c^2_i}{n_i}$$

We can test $$H_0: \sum c_i = 0$$ with 

$$T = \frac{\sum c_i \bar{y}_{i.}}{s_{\epsilon} \sqrt{\sum c_i^2 / n_i}}$$

which is distributed $$t_{dfE}$$ (two-sided test). Similarly a $$95$$% confidence interval can be created.

Note that when we have an ANOVA with $$k$$ treatments and a set of $$k - 1$$ orthogonal contrasts (ie $$c_i c_j = 0$$), then the SS will add up to $$SSTrt$$. One example of a set of orthogonal contrasts are linear, quadratic, cubic, etc contrasts.

When one is assessing multiple contrasts, it would be wise to control for [multiple comparisons][multiple_comp_link]{:target = "_blank"}. 

In R, we can use `pairwise.t.test(y, x)`, `p.adjust()`. We can also test contrasts using the `multcomp::glht()` function.

## Unbalanced ANOVAs

How do we assess effects when our designs are missing certain factor combinations? For this, contrasts can come in handy.

* Fit a one-way ANOVA as a combination of the factors 
* Fit a contrast to assess the effect you would like (perhaps the interaction)
* Use a MSE from the one-way ANOVA as an estimate of $$s^2_{\epsilon}$$

# Nonparametric Tests

## Ranked ANOVA
A nonparametric alternative to ANOVA requires a rank transformation. The procedure for this method is listed below.

1. Rank data set from largest to smallest
2. Analyze rank values in standard ANOVA

### Kruskal-Wallis Test
For this test, we make the following assumptions

* Independent samples
* Continuous variable
* Equal variances
* Identical (but non-normal) distributions

The steps for this test is as follows

1. Rank the combined data
2. Record the mean ranks for each group $$\bar{R}_i$$
3. Compute the test statistic

$$KW = (N - 1) \frac{\sum^k_{i = 1} n_i (\bar{R}_{i.} - \bar{R})^2}{\sum^k_{i = 1} \sum^{n_i}_{j = 1} (R_{ij} - \bar{R})^2}$$

where $$KW$$ is approximately distributed $$X^2_{k - 1}$$. When sample sizes are small, we can compare to permutations or distribution tables. 

In R, we fit with `kruskal.test()`.

## Friedman's Test
The nonparametric equivalent for the two-factor ANOVA is Friedman's test. We test the null hypothesis that each rank within each block is equally likely. 

The steps for this test is as follows

1. Rank the observations within each block
2. Record the mean ranks for each group $$\bar{R}_i$$ across blocks
3. Compute the test statistic

$$Q = N^2(k - 1) \frac{\sum^k_{i = 1} (\bar{R}_{i.} - \bar{R})^2}{\sum^k_{i = 1} \sum^{n_i}_{j = 1} (R_{ij} - \bar{R})^2}$$

where $$Q$$ is approximately distributed $$X^2_{k - 1}$$. When sample sizes are small, we can compare to permutations or distribution tables. 

In R, we fit with `friedman.test()`.

# Tests of Equal Variance
**Levene's Test**

Levene's test is a formal test to assess $$H_0: \sigma^2_1 = ... = \sigma^2_k$$. 

Procedure:

1. Let $$d_{ij} = \| y_{ij} - \tilde{y_i} \|$$ where $$\tilde{y_i}$$ is the median of group $$i$$
2. Perform a one-way ANOVA on the $$d_{ij}$$
3. Reject $$H_0: \sigma^2_1 = ... = \sigma^2_k$$ if the $$F$$-test is significant

[stat_theory_link]: http://jnguyen92.github.io/nhuyhoa//2015/10/Distribution-Facts.html
[multiple_comp_link]: http://jnguyen92.github.io/nhuyhoa//2016/02/Multiple-Comparisons.html
