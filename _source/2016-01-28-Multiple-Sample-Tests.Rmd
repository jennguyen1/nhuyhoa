---
layout: post
title: "Multiple Sample Tests"
date: "January 28, 2016"
categories: ['statistics', 'experimental design']
---

* TOC
{:toc}

```{r, echo = FALSE}
library(jn.general)
lib(data, viz)
knitr::opts_chunk$set(fig.width = 5, fig.height = 5, fig.align = 'center')
```

# ANOVA
In R, ANOVA can be fit with the `lm()` and `anova()` or `aov()` commands.

**Assumptions**

Regardless of the type, ANOVAs have the same assumptions:

1. Independence: within and across treatments
2. Normality: $$Y_{ij}$$ ~ $$N(\mu_i, \sigma^2_i)$$
3. Equal Variance: $$\sigma^2_1 = ... = \sigma^2_k$$

## Factorial Designs

### Completely Randomized Single Factor ANOVA 

**Model Formulations**

Consider a design with $$k$$ groups where there are $$n_i$$ observations on the $$i^{th}$$ treatment. Let $$y_{ij}$$ denote the $$j^{th}$$ observation on the $$i^{th}$$ treatment. 

$$Y_{ij} = \mu_i + \epsilon_{ij}$$ 

where $$\epsilon_{ij}$$ ~ iid$$N(0, \sigma^2_{\epsilon})$$

$$Y_{ij} = \mu + \alpha_i + \epsilon_{ij}$$ 

where $$\epsilon_{ij}$$ ~ iid$$N(0, \sigma^2_{\epsilon})$$. We assume $$\sum \alpha_i = 0$$ if $$H_0$$ is true.

**ANOVA Table**

We have the following terms

* treatment sum: $$y_{i.} = \sum^{n_i}_{j = 1} y_{ij}$$
* treatment mean: $$\bar{y}_{i.} = y_{i.}/n_i$$
* overall sum: $$y_{..} = \sum^k_{i = 1} \sum^{n_i}_{j = 1} y_{ij}$$
* overall mean: $$\bar{y}_{..} = y_{..} / N$$

Source| Sum of Squares | Degrees of Freedom | Mean Square | F 
------|----------------|--------------------|-------------|---------
Trt   | $$\sum^k_{i = 1} n_i(\bar{y}_{i.} - \bar{y}_{..})^2$$ | $$k-1$$ | $$\frac{SSTrt}{dfTrt}$$ | $$\frac{MSTrt}{MSE}$$ 
Error | $$\sum^k_{i = 1} \sum^{n_i}_{j = 1} (y_{ij} - \bar{y}_{i.})^2 = \sum^k_{i = 1} (n_i - 1) s_i^2$$ | $$N-k$$ | $$\frac{SSE}{dfE}$$ |
Total | $$\sum^k_{i = 1} \sum^{n_i}_{j = 1} (y_{ij} - \bar{y}_{..})^2 = \sum_{all.obs} (y_{ij} - \bar{y}_{..})$$ | $$N-1$$ | | 

<p></p>
 
where $$F$$ ~ $$F_{dfTrt, dfErr}$$. When $$F$$ is large, we say that the group effect is large so we reject the null hypothesis that the groups are the same.

We can derive this distribution from [probability theory][stat_theory_link]{:target = "_blank"}. The value $$\frac{df * MS}{\sigma^2}$$ is distributed $$X^2$$. So 

$$F = \frac{ \frac{df * MS}{\sigma^2} / df }{ \frac{dfErr * MSErr}{\sigma^2} / dfErr } = \frac{ MSTrt }{ MSErr }$$ 

is distributed $$F_{df, dfErr}$$. 

Another way to look at this is to consider if $$H_0$$ is true

* .$$\sum_i \alpha_i = 0$$
* .$$E[MSTrt] = \sigma^2_{\epsilon} + \frac{n}{k - 1} \sum^k_i \alpha_i^2$$
* .$$E[MSE] = \sigma^2_{\epsilon}$$

Then 

$$\frac{E[MSTrt]}{E[MSE]} = 1 + \frac{1}{k - 1} \frac{n \sum^k_i \alpha^2_i}{\sigma^2_{\epsilon}}$$ 

Note that if $$\sum \alpha_i \approx 0$$, then this ratio $$\approx 1$$. The power of the $$F$$ test is a monotone function of $$\frac{n \sum^k_i \alpha^2_i}{\sigma^2_{\epsilon}}$$.

### Completely Randomized Two Factor ANOVA

**Model Formulations**

$$Y_{ijl} = \mu + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ijl}$$

where

* $$i = 1, ..., a$$ denotes the levels of factor A
* $$j = 1, ..., b$$ denotes the levels of factor B
* $$l = 1, ..., n$$ denotes replicates of each factor combination
* $$\epsilon_{ijl}$$ ~ $$N(0, \sigma^2_{\epsilon})$$ represents the plot error


**ANOVA Table**

Source| Sum of Squares | Degrees of Freedom 
------|----------------|--------------------
A     | $$bn\sum^a_{i = 1} (\bar{y}_{i..} - \bar{y}_{...})^2$$ | $$a-1$$ 
B     | $$an\sum^b_{j = 1} (\bar{y}_{.j.} - \bar{y}_{...})^2$$ | $$b-1$$ 
AB    | $$n \sum^a_{i = 1} \sum^b_{j = 1} (\bar{y}_{ij.} - \bar{y}_{i..} - \bar{y}_{.j.} + \bar{y}_{...})^2$$ | $$(a-1)(b-1)$$ 
Error | $$\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{l = 1} (y_{ijl} - \bar{y}_{ij.})^2$$ | $$ab(n - 1)$$ 
Total | $$\sum^a_{i = 1} \sum^b_{j = 1} \sum^n_{l = 1} (y_{ijl} - \bar{y}_{...})^2$$ | $$abn - 1$$ 

<p></p>

Note that we have three separate $$F$$ tests with the error as the demonimator (similar to the single factor case) to assess the $$A_{main}$$, $$B_{main}$$, and $$AB_{int}$$ effects. 

Assume $$H_0$$ is true so that

* $$\sum_i \alpha_i = 0$$, $$\sum_j \beta_j = 0$$ and $$\sum_i (\alpha \beta)_{ij} = \sum_j (\alpha \beta)_{ij} = 0$$
* .$$E[MSA] = \sigma^2_{\epsilon} + \frac{bn}{a - 1} \sum \alpha^2_i$$
* .$$E[MSB] = \sigma^2_{\epsilon} + \frac{an}{b - 1} \sum \beta^2_j$$
* .$$E[MSAB] = \sigma^2_{\epsilon} + \frac{n}{(a - 1)(b - 1)} \sum (\alpha \beta)^2_{ij}$$
* .$$E[MSE] = \sigma^2_{\epsilon}$$

We can test the main effects and interaction by comparing them to $$MSE$$ with an $$F$$ test.

**Contrasts**

Let $$\bar{ab}$$ denote the mean of the high-high group, $$\bar{a}$$ denote the mean of high-low group, $$\bar{b}$$ denote the mean of the low-high group, and $$\bar{1}$$ denote the mean of the low-low group. We have

* .$$A_{main} = \frac{1}{2} [(\bar{ab} - \bar{b}) + (\bar{a} - \bar{1})]$$
* .$$B_{main} = \frac{1}{2} [(\bar{ab} - \bar{a}) + (\bar{b} - \bar{1})]$$
* .$$AB_{int} = \frac{1}{2} [(\bar{ab} - \bar{b}) - (\bar{a} - \bar{1})]$$

We can assess these effects by generating an interaction plot. By assessing the trends and parallelism of lines, we can determine whether there may be evidence of an interaction effect. We can also test these effects with a contrast test.

### Completely Randomized Three Factor ANOVA

**Model Formulations**

$$Y_{ijkl} = \mu + \alpha_i + \beta_j + \gamma_k + (\alpha \beta)_{ij} + (\alpha \gamma)_{ik} + (\beta \gamma)_{jk} \epsilon_{ijkl}$$

where

* $$i = 1, ..., a$$ denotes the levels of factor A
* $$j = 1, ..., b$$ denotes the levels of factor B
* $$k = 1, ..., c$$ denotes the levels of factor C
* $$l = 1, ..., n$$ denotes replicates of each factor combination
* $$\epsilon_{ijkl}$$ ~ $$N(0, \sigma^2_{\epsilon})$$ represents the plot error

**ANOVA Table**

Source| Sum of Squares | Degrees of Freedom 
------|----------------|--------------------
A     | $$bcn\sum^a_{i = 1} (\bar{y}_{i...} - \bar{y}_{....})^2$$ | $$a-1$$ 
B     | $$acn\sum^b_{j = 1} (\bar{y}_{.j..} - \bar{y}_{....})^2$$ | $$b-1$$ 
C     | $$abn\sum^b_{k = 1} (\bar{y}_{..k.} - \bar{y}_{....})^2$$ | $$c-1$$
AB    | $$cn \sum^a_{i = 1} \sum^b_{j = 1} (\bar{y}_{ij..} - \bar{y}_{i...} - \bar{y}_{.j..} + \bar{y}_{....})^2$$ | $$(a-1)(b-1)$$ 
AC    | $$bn \sum^a_{i = 1} \sum^c_{k = 1} (\bar{y}_{i.k.} - \bar{y}_{i...} - \bar{y}_{..k.} + \bar{y}_{....})^2$$ | $$(a-1)(c-1)$$ 
BC    | $$an \sum^b_{j = 1} \sum^c_{k = 1} (\bar{y}_{.jk.} - \bar{y}_{.j..} - \bar{y}_{..k.} + \bar{y}_{....})^2$$ | $$(b-1)(c-1)$$ 
ABC   | $$n \sum_{ijkl} (y_{ijk.} - \bar{y}_{ij..} - \bar{y}_{i.k.} - \bar{y}_{.jk.} + \bar{y}_{i...} + \bar{y}_{.j..} + \bar{y}_{..k.} - \bar{y}_{....})^2$$ | $$(a-1)(b-1)(c-1)$$ 
Error | $$\sum_{ijkl} (y_{ijkl} - \bar{y}_{ijk.})^2$$ | $$abc(n - 1)$$ 
Total | $$\sum_{ijkl} (y_{ijk.} - \bar{y}_{...})^2$$ | $$abcn - 1$$ 

<p></p>

Note that we have many separate $$F$$ tests with the error as the demonimator (similar to the single factor case).

We need to consider the hierarchy principal when we consider these effects. If a higher order term is important, that means that a lower order term is important as well (and as in linear regression, should not be removed from the model). 

**Contrasts**

For contrasts, we have

* .$$A_{main} = \frac{1}{4} [(\bar{abc} - \bar{bc}) + (\bar{ab} - \bar{b}) + (\bar{ac} - \bar{c}) + (\bar{a} - \bar{1})]$$
* .$$B_{main} = \frac{1}{4} [(\bar{abc} - \bar{ac}) + (\bar{ab} - \bar{a}) + (\bar{bc} - \bar{c}) + (\bar{b} - \bar{1})]$$
* .$$C_{main} = \frac{1}{4} [(\bar{abc} - \bar{ab}) + (\bar{ac} - \bar{a}) + (\bar{bc} - \bar{b}) + (\bar{c} - \bar{1})]$$

* .$$AB_{int} = \frac{1}{4} [(\bar{abc} - \bar{bc}) - (\bar{ac} - \bar{c}) + (\bar{ab} - \bar{b}) - (\bar{a} - \bar{1})]$$
* .$$AC_{int} = \frac{1}{4} [(\bar{abc} - \bar{bc}) - (\bar{ab} - \bar{b}) + (\bar{ac} - \bar{c}) - (\bar{a} - \bar{1})]$$
* .$$BC_{int} = \frac{1}{4} [(\bar{abc} - \bar{ac}) - (\bar{ab} - \bar{a}) + (\bar{bc} - \bar{c}) - (\bar{b} - \bar{1})]$$

* .$$ABC_{int} = \frac{1}{4} [(\bar{abc} - \bar{bc}) - (\bar{ac} - \bar{c}) - (\bar{ab} - \bar{b}) - (\bar{a} - \bar{1})]$$

We can assess these effects by generating an interaction plot. We can also test these effects with a contrast test.

### Randomized Complete Block Design (One Factor) ANOVA

Blocking is a generalization of the paired analysis in t-tests. We expect there to be block to block variability. The observations in the same block share a block effect and are less variable than observations in other blocks. By accounting for the block variability, we decrease the unaccounted variability and make the test for treatment more powerful. 

A single block should be homogeneous. To ensure this homogeneity, we want to make sure blocks are small enough that

* all treatments are contained with a block but
* there are no duplicated treatments within a block

**Model Formulations**

$$Y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}$$

where

* $$i = 1, ..., k$$ denotes the levels of treatment
* $$j = 1, ..., b$$ denotes the levels of blocks
* $$\epsilon_{ij}$$ ~ $$N(0, \sigma^2_{\epsilon})$$ represents the plot error

**ANOVA Table**

Source| Sum of Squares | Degrees of Freedom 
------|----------------|--------------------
Blocks| $$k\sum^b_{j = 1} (\bar{y}_{.j} - \bar{y}_{..})^2$$ | $$b-1$$ 
Trt   | $$b\sum^k_{i = 1} (\bar{y}_{i.} - \bar{y}_{..})^2$$ | $$k-1$$ 
Error | $$\sum_{ij} (y_{ij} - \bar{y}_{i.} - \bar{y}_{.j} + \bar{y}_{..})^2$$ | $$(k-1)(b-1)$$ 
Total | $$\sum_{ij} (y_{ij} - \bar{y}_{..})^2$$ | $$kb - 1$$ 

<p></p>

We can assess the effect of treatment with an $$F$$ test with the error in the denominator. 

Note that we can ignore the $$F$$ test for the effect of blocks. The blocks were used in order to account for variability. A significant test would indicate that we were right to block. If we did not find a significant block effect, we should not pool the block with error and analyze the data as a completely randomized design! We need to analyze the data as we have designed it, otherwise we would bias our tests. 

We notice several things from this table. One is that this table is reminiscent of the ANOVA table for two-factor ANOVA where $$n = 1$$. Another similarity is that the SS for error is similar to the SS for the interaction term in the two-factor case. This is because we assume an additive model and so the error that is normally given to the interaction is transferred to the error.

Thus RCBD with one factor is the same as a completely randomized two-factor ANOVA where $$n = 1$$.

### Randomized Complete Block Design (Two Factor) ANOVA

$$Y_{ijkl} = \mu + \alpha_i + \beta_j + \gamma_k + (\alpha \beta)_{ij} + (\alpha \gamma)_{ik} + (\beta \gamma)_{jk} \epsilon_{ijkl}$$


**Model Formulations**

$$Y_{ijk} = \mu + \alpha_i + \beta_k +  \gamma_j + (\alpha \gamma)_{ij} +
\epsilon_{ij}$$

where

* $$i = 1, ..., a$$ denotes the levels of factor A
* $$j = 1, ..., b$$ denotes the levels of blocks
* $$k = 1, ..., c$$ denotes the levels of factor C
* $$\epsilon_{ij}$$ ~ $$N(0, \sigma^2_{\epsilon})$$ represents the plot error

**ANOVA Table**

Source| Degrees of Freedom 
------|---------------------
Blocks| $$b-1$$
A     | $$a-1$$
C     | $$c-1$$
AC    | $$(a-1)(c-1)$$
Error | $$(b-1)(ac-1)$$

<p></p>

### Latin Squares

Latin squars are experimental designs that block in two directions. These designs require the number of row blocks = number of column blocks = number of treatment levels. 

In Latin Square designs, there are not values for every combination of $$i$$, $$j$$, $$k$$. The means include only the $$y_{ijl}$$ terms that exist. There will be $$k^2$$ of them. 

**Model Formulations**

$$Y_{ijl} = \mu + \alpha_i + r_j + c_l + \epsilon_{ij}$$

where

* $$i = 1, ..., k$$ denotes the levels of treatment
* $$j = 1, ..., k$$ denotes the levels of row blocks
* $$l = 1, ..., k$$ denotes the levels of column blocks
* $$\epsilon_{ij}$$ ~ $$N(0, \sigma^2_{\epsilon})$$ represents the plot error

**ANOVA Table**

Source| Sum of Squares | Degrees of Freedom 
------|----------------|--------------------
Row   | $$k \sum^k_j (\bar{y}_{.j.} - \bar{y}_{...})^2$$ | $$k-1$$ 
Column| $$k \sum^k_l (\bar{y}_{..l} - \bar{y}_{...})^2$$ | $$k-1$$ 
Trt   | $$k \sum^k_i (\bar{y}_{i..} - \bar{y}_{...})^2$$ | $$k-1$$ 
Error | By subtraction | $$(k-1)(k-2)$$ 
Total | $$\sum_{ijl} (y_{ijl} - \bar{y}_{...})^2$$ | $$k^2 - 1$$ 

<p></p>

We can assess the effect of treatment with an $$F$$ test with the error in the denominator. 

## Random Effects and Mixed Models

### Simple Random Effect Model

**Model Formulations**

$$Y_{ij} = \mu + A_i + \epsilon_{ij}$$

where

* $$i = 1, ..., k$$ denotes the levels of treatment
* $$j = 1, ..., b$$ denotes the experimental units for each treatment
* $$A_i$$ ~ $$N(0, \sigma^2_A)$$ corresponds to the random effect (group variation)
* $$\epsilon_{ij}$$ ~ $$N(0, \sigma^2_{\epsilon})$$ represents the error within each group

Note that this is quite similar to the fixed effects (previous models) that were covered. However, $$A_i$$ represents a sample from some population $$N(0, \sigma^2_A)$$ which we are interested in (rather than specific distinct group). 


**ANOVA Table**

Source| Sum of Squares | Degrees of Freedom | Mean Square | E[MS]
------|----------------|--------------------|-------------|---------
Trt   | $$\sum^k_{i = 1} n_i(\bar{y}_{i.} - \bar{y}_{..})^2$$ | $$k-1$$ | $$MSTrt$$ | $$\sigma^2_{\epsilon} + n \sigma^2_A$$ 
Error | $$\sum^k_{i = 1} \sum^{n_i}_{j = 1} (y_{ij} - \bar{y}_{i.})^2 = \sum^k_{i = 1} (n_i - 1) s_i^2$$ | $$k(n-1)$$ | $$MSE$$ | $$\sigma^2_{\epsilon}$$
Total | $$\sum_{ij} (y_{ij} - \bar{y}_{..})^2 = \sum_{all.obs} (y_{ij} - \bar{y}_{..})$$ | $$kn-1$$ | | 

<p></p>

We test $$H_0: \sigma^2_A = 0$$ vs. $$H_A: \sigma^2_A > 0$$ with $$F = \frac{MSTrt}{MSE}$$. 

Based off this ANOVA table, we use $$MSE$$ to estimate $$\sigma^2_{\epsilon}$$ and $$\frac{MSTrt - MSE}{n}$$ to estimate $$\sigma^2_A$$. 

Note that a confidence interval would be $$\bar{y}_{..} \pm t_{\alpha/2, k-1} \sqrt{MSTrt/(nk)}$$.

We can use the expected MS to find the variance of $$Y$$.

----------------|----------------
$$Var(Y_{ij})$$ | $$= \sigma^2_A + \sigma^2_{\epsilon}$$
$$Var(\bar{y}_{..})$$ | $$= Var(\mu + \hat{A}_. + \hat{\epsilon})$$
                | $$=\frac{\sigma^2_A}{k} + \frac{\sigma^2_{\epsilon}}{nk}$$
                | $$=\frac{n\sigma^2_A + \sigma^2_{\epsilon}}{nk}$$
                
Note that $$Var(\bar{y}_{..})$$ $$\rightarrow 0$$ as $$k \rightarrow \infty$$ and $$\rightarrow \sigma^2_{\epsilon} / k$$ as $$n \rightarrow \infty$$. 

Another important quantity is the intraclass correlation coefficient

$$ICC = \frac{s^2_{b/n trt}}{s^2_{b/n trt} + s^2_{within trt}}$$

This value is the correlation between the observations within the group. Small values indicate large spared of values at each level of treatment. Large values indicate little spread at each level of treatment.

### Models with Subsampling

To understand models with subsampling, we define the following:

An **experimental unit** is the group to which a treatment is applied in a single trial of the experiment. We may also call an experimental unit "a plot". 

A **sampling unit** is a subunit within an experimental unit. When analyzing experimental data, we either have to adjust for a sampling unit in the model or average sampling units for each experimental unit prior to analysis. Treating sampling units as experimental units can inflate the error df. 

Consider a one-sample test with subsampling. The model is written as

$$Y_{ij} = \mu + \epsilon_i + \delta_{ij}$$ 

where $$\epsilon_i$$ ~ $$N(0, \sigma^2_{\epsilon})$$ and $$\delta_{ij}$$ ~ $$N(0, \sigma^2_{\delta})$$. In this setting $$\epsilon$$ represents the experimental units, while $$\delta$$ represents the subsamples.

One interesting note is that this model will give equivalent results if one were to average the subsamples (technical replicates) and run a regular ANOVA. In other words,

$$\bar{Y}_{ij.} = \mu + \alpha_i + \epsilon_{ij} + \bar{\delta}_{ij.} = \mu + \alpha_i + e_{ij}$$

Models with subsampling are essentially the same as models that average those subsamples for each experimental unit. Thus even though the $$SS$$ and $$MS$$ for these models are different, the $$F$$ statistic, degrees of freedom, and p-value for effects are all the same. If we decide not to average subsamples, we need to appropriately account for them. 

### Completely Randomized Design with Subsampling

Consider an experiment where the treatments are fixed but we take subsamples which are random. This is an example of a mixed model.

**Model Formulations**

$$Y_{ij} = \mu + \alpha_i + \epsilon_{ij} + \delta_{ijl}$$

where

* $$i = 1, ..., k$$ denotes the levels of treatment
* $$j = 1, ..., b$$ denotes the experimental units for each treatment
* $$l = 1, ..., s$$ denotes the subsample within each experimental unit
* $$\epsilon_{ij}$$ ~ $$N(0, \sigma^2_{\epsilon})$$ corresponds to the error within experimental units
* $$\delta{ijl}$$ ~ $$N(0, \sigma^2_{\delta})$$ represents the subsample error

**ANOVA Table**

Source| Sum of Squares | Degrees of Freedom | Mean Square | E[MS]
------|----------------|--------------------|-------------|---------
Trt   | $$sn \sum^k_{i} (\bar{y}_{i..} - \bar{y}_{...})^2$$ | $$k-1$$ | $$MSTrt$$ | $$\sigma^2_{\delta} + s \sigma^2_{\epsilon} + ns \sum^k_i \frac{\alpha_i^2}{k - 1}$$ 
Plot Error | $$s \sum^k_{i} \sum^{n}_{j} (\bar{y}_{ij.} - \bar{y}_{i..})^2$$ | $$k(n-1)$$ | $$MSPE$$ | $$\sigma^2_{\delta} + s\sigma^2_{\epsilon}$$
Subsample Error | $$\sum_{ijl} (y_{ijl} - \bar{y}_{ij.})^2$$ | $$kn(s-1)$$ | $$MSSSE$$ | $$\sigma^2_{\delta}$$
Total | $$\sum_{ijl} (y_{ijl} - \bar{y}_{...})^2$$ | $$kns-1$$ | | 

<p></p>

Notice that the expected MS are a little different. To conduct our $$F$$ test for treatment, we compute $$F = \frac{MSTrt}{MSPE}$$, where $$E[F] = 1 + 1 + ns \sum^k_i \alpha^2_i / (k - 1)$$. 

The guideline here is that when testing any source of variability for significance, we look for a denominator for the $$F$$ test that contains all the elements of the $$E[MS]$$ except the source of interest.

### Randomized Complete Block Design with Subsampling

**Model Formulations**

$$Y_{ijl} = \mu + \alpha_i + \beta_j + \epsilon_{ij} + \delta_{ijl}$$

where

* $$i = 1, ..., k$$ denotes the levels of treatment
* $$j = 1, ..., b$$ denotes the blocks
* $$l = 1, ..., s$$ denotes the subsample within each plot
* $$\epsilon_{ij}$$ ~ $$N(0, \sigma^2_{\epsilon})$$ corresponds to the plot error
* $$\delta{ijl}$$ ~ $$N(0, \sigma^2_{\delta})$$ represents the subsample error

**ANOVA Table**

Source| Sum of Squares | Degrees of Freedom | Mean Square | E[MS]
------|----------------|--------------------|-------------|---------
Blocks| $$ks \sum_j (\bar{y}_{.j.} - \bar{y}_{...})^2$$ | $$b-1$$ | $$MSBlk$$ | $$\sigma^2_{\delta} + s \sigma^2_{\epsilon} + ks \sum^b_j \frac{\beta_j^2}{b - 1}$$ 
Trt   | $$bs \sum^k_{i} (\bar{y}_{i..} - \bar{y}_{...})^2$$ | $$k-1$$ | $$MSTrt$$ | $$\sigma^2_{\delta} + s \sigma^2_{\epsilon} + bs \sum^k_i \frac{\alpha_i^2}{k - 1}$$ 
Plot Error | $$s \sum_{ij} (\bar{y}_{ij.} - \bar{y}_{i..} + \bar{y}_{.j.} +\bar{y}_{...})^2$$ | $$(k-1)(b-1)$$ | $$MSPE$$ | $$\sigma^2_{\delta} + s\sigma^2_{\epsilon}$$
Subsample Error | $$\sum_{ijl} (y_{ijl} - \bar{y}_{ij.})^2$$ | $$kb(s-1)$$ | $$MSSSE$$ | $$\sigma^2_{\delta}$$
Total | $$\sum_{ijl} (y_{ijl} - \bar{y}_{...})^2$$ | $$kbs-1$$ | | 

<p></p>

### Many Options of Mixed Models

We can have any number of designs (fixed, random, mixed, factorial, nested) with any number of factors. Different designs will lead to different $$F$$ tests. It is important to look at $$E[MS]$$ to determine the appropriate $$F$$ test construction. 

# Pairwise Comparisons and Contrasts

After rejecting a test that the means of the groups are not equal, we want to know exactly which ones are different. 

A contrast is a linear function of the group means. It can be used to compare two means or any set of groups of groups.

Procedure:

* An arbitrary contrast $$C = \Sigma^r_{i = 1} c_i \mu_i$$ where $$\Sigma^r_{i = 1} c_i = 0$$. There can be an infinite number of contrasts.
* $$C$$ is estimated with $$\hat{C} = \Sigma^r_{i = 1} c_i \bar{Y}_i$$ 
* The variance of the estimate $$\hat{C}$$ is $$s_{\hat{C}}^2 = \sigma^2_e \Sigma^r_{i = 1} \frac{c^2_i}{n_i}$$

We can test $$H_0: \sum c_i = 0$$ with 

$$T = \frac{\sum c_i \bar{y}_{i.}}{s_{\epsilon} \sqrt{\sum c_i^2 / n_i}}$$

which is distributed $$t_{dfE}$$ (two-sided test). Similarly a $$95$$% confidence interval can be created.

Note that when we have an ANOVA with $$k$$ treatments and a set of $$k - 1$$ orthogonal contrasts (ie $$c_i c_j = 0$$), then the SS will add up to $$SSTrt$$. One example of a set of orthogonal contrasts are linear, quadratic, cubic, etc contrasts.

When one is assessing multiple contrasts, it would be wise to control for [multiple comparisons][multiple_comp_link]{:target = "_blank"}. 

In R, we can use `pairwise.t.test(y, x)`, `p.adjust()`. We can also test contrasts using the `multcomp::glht()` function.

## Unbalanced ANOVAs

How do we assess effects when our designs are missing certain factor combinations? For this, contrasts can come in handy.

* Fit a one-way ANOVA as a combination of the factors 
* Fit a contrast to assess the effect you would like (perhaps the interaction)
* Use a MSE from the one-way ANOVA as an estimate of $$s^2_{\epsilon}$$

# Nonparametric Tests

## Ranked ANOVA
A nonparametric alternative to ANOVA requires a rank transformation. The procedure for this method is listed below.

1. Rank data set from largest to smallest
2. Analyze rank values in standard ANOVA

### Kruskal-Wallis Test
For this test, we make the following assumptions

* Independent samples
* Continuous variable
* Equal variances
* Identical (but non-normal) distributions

The steps for this test is as follows

1. Rank the combined data
2. Record the mean ranks for each group $$\bar{R}_i$$
3. Compute the test statistic

$$KW = (N - 1) \frac{\sum^k_{i = 1} n_i (\bar{R}_{i.} - \bar{R})^2}{\sum^k_{i = 1} \sum^{n_i}_{j = 1} (R_{ij} - \bar{R})^2}$$

where $$KW$$ is approximately distributed $$X^2_{k - 1}$$. When sample sizes are small, we can compare to permutations or distribution tables. 

In R, we fit with `kruskal.test()`.

## Friedman's Test
The nonparametric equivalent for the two-factor ANOVA is Friedman's test. We test the null hypothesis that each rank within each block is equally likely. 

The steps for this test is as follows

1. Rank the observations within each block
2. Record the mean ranks for each group $$\bar{R}_i$$ across blocks
3. Compute the test statistic

$$Q = N^2(k - 1) \frac{\sum^k_{i = 1} (\bar{R}_{i.} - \bar{R})^2}{\sum^k_{i = 1} \sum^{n_i}_{j = 1} (R_{ij} - \bar{R})^2}$$

where $$Q$$ is approximately distributed $$X^2_{k - 1}$$. When sample sizes are small, we can compare to permutations or distribution tables. 

In R, we fit with `friedman.test()`.

# Tests of Equal Variance
**Levene's Test**

Levene's test is a formal test to assess $$H_0: \sigma^2_1 = ... = \sigma^2_k$$. 

Procedure:

1. Let $$d_{ij} = \| y_{ij} - \tilde{y_i} \|$$ where $$\tilde{y_i}$$ is the median of group $$i$$
2. Perform a one-way ANOVA on the $$d_{ij}$$
3. Reject $$H_0: \sigma^2_1 = ... = \sigma^2_k$$ if the $$F$$-test is significant

[stat_theory_link]: http://jnguyen92.github.io/nhuyhoa//2015/10/Distribution-Facts.html
[multiple_comp_link]: http://jnguyen92.github.io/nhuyhoa//2016/02/Multiple-Comparisons.html
