---
layout: post
title: "Support Vector Machines (SVM)"
date: "December 30, 2015"
categories: ['statistics', 'machine learning']
---

* TOC
{:toc}

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(jn.general)
lib(data, viz)
knitr::opts_chunk$set(fig.width = 5, fig.height = 5, fig.align = 'center')
```

For binary classification problems, we can use support vector machines to find a plane that separates classes in the feature space. Note that we should standardize our features prior to running SVM. 

# Separating Hyperplane
Our separating hyperplane has the form <br>
$$f(\overrightarrow{x}) =  b + \beta_1 X_1 + ... + \beta_p X_p = 0$$ 

or in vector notation <br>
$$f(\overrightarrow{x}) = \overrightarrow{\beta}^T \overrightarrow{x} + b$$

The vector $$\overrightarrow{\beta}$$ is the normal vector: it is orthogonal to the hyperplane. 

We can classify a data point based on $$f(\overrightarrow{x})$$:

* group $$A$$ if $$f(\overrightarrow{x}) > 0$$
* group $$B$$ if $$f(\overrightarrow{x}) < 0$$

We can also use the magnitude of $$f(\overrightarrow{x})$$ to assess our confidence about the class assignment. 

* more confident the greater the magitude of $$f(\overrightarrow{x})$$ is (farther from separator)
* less confident the smaller the magnitude of $$f(\overrightarrow{x})$$ is (closer to separator)

We code the points as $$Y_i = 1$$ or $$Y_i = -1$$, so that $$Y_i * f(X_i) > 0 \forall i$$. 

# Maximal Margin Classifier
We can find the best separating hyperplane with the maximal margin classifier. In other words, we want to biggest gap or margin between the two classes. 

![maximal margin classifier](http://jnguyen92.github.io/nhuyhoa/figure/images/maximal_margin_classifier2.png)

Schematic diagram of maximal margin classifier. The margin is defined as $$M = \frac{2}{\| \overrightarrow{\beta} \|}$$.

This is an optimization problem: <br>
$$maximize_{\beta_0, \beta_1, ..., \beta_p} M > 0$$ <br>
w.r.t. $$\| \overrightarrow{\beta} \|= 1$$ <br>
so that $$y_i (\overrightarrow{\beta}^T \overrightarrow{x_i}+b) \ge M \forall i$$

This is equivalent to minimizing $$\| \overrightarrow{\beta} \|$$ or $$\| \overrightarrow{\beta} \|^2$$.

(This problem can be solved with linear or quadratic programming).

We see from the figure below that there are 3 training observations are are equidistant from the separating hyperplane. These three observations are called support vectors. The maximal margin classifier depends directly on the support vectors (and not on other observations far away from it).

![maximal margin classifier 2](http://jnguyen92.github.io/nhuyhoa/figure/images/maximal_margin_classifier.png)

A schematic diagram of maximum margin classifier (Hastie, et. al).

# Slack Variables and C Boundary
The separating hyperplane is sensitive to the support vectors, and thus may be overfitting the data. Thus to accommodate this issue, one maximize a soft margin. 

We introduce slack variables,  $$\epsilon_i$$, that tell us where the $$i^{th}$$ observation is located relative to the hyperplane and margins. 

* $$\epsilon_i = 0$$ on the correct side of margin
* $$\epsilon_i > 0$$ on the wrong side of the margin
* $$\epsilon_i > 1$$ on the wrong side of the hyperplane

![slack variables](http://jnguyen92.github.io/nhuyhoa/figure/images/svm_slack_variables.png)

The new optimization problem becomes <br>
$$maximize_{\beta_0, \beta_1, ..., \beta_p, \epsilon_1, ..., \epsilon_n} M > 0$$ <br>
w.r.t. $$\| \overrightarrow{\beta} \|= 1$$ <br>
so that $$y_i (\overrightarrow{\beta}^T \overrightarrow{x}+b) \ge M(1 - \epsilon_i) \forall i$$ <br>
where $$\epsilon_i \ge 0$$ and $$\sum^n_{i = 1} \epsilon_i \le C$$

This is equivalent to <br>
$$min \| \overrightarrow{\beta} \| + C \sum^N_i \epsilon_i$$ with the constraints <br>
$$\beta^T x_k + b \ge 1 + \epsilon_k$$ if $$y_k = 1$$ <br>
$$\beta^T x_k + b \le -1 + \epsilon_k$$ if $$y_k = -1$$ <br>
$$\epsilon_k \ge 0 \forall k$$


The variable $$C$$ is a penalty boundary chosen via cross-validation. It determines the number and severity of violations to the margin and the hyperplane. 

* Small $$C$$ allows for wider margins and is more tolerant of violations
* Large $$C$$ forces narrower margins that are rarely violated

![C penalty](http://jnguyen92.github.io/nhuyhoa/figure/images/svm_C_parameter.png)

A schematic diagram of the effects of $$C$$ penalty from small $$C$$ (upper left) to big $$C$$ (lower right) (Hastie, et. al).

# Kernals & Mapping to Higher Dimensions
In some cases, our data points are non-linearly separable. For these instances, we would need to transform our features by mapping to a higher dimensional space. With our expanded dimensional space, we can sufficiently fit a linear separator to divide our data. 

We can use kernals to map to a higher dimensional space. Kernals have the form $$K(x_i, x_j) = \phi(x_i)^T \phi(x_j)$$.

* Linear: $$K(x_i, x_j) = x_i^T x_j$$
* Polynomial: $$K(x_i, x_j) = (\gamma x_i^T x_j + r)^d$$, $$\gamma > 0$$
* Radial basis function: $$K(x_i, x_j) = exp(-\gamma \| x_i - x_j \|^2)$$, $$\gamma > 0$$
* Sigmoid: $$K(x_i, x_j) = tanh(\gamma x_i^T x_j + r)$$

![polynomial kernal](http://jnguyen92.github.io/nhuyhoa/figure/images/svm_nonlinear_kernal1.png)![radial kernal](http://jnguyen92.github.io/nhuyhoa/figure/images/svm_nonlinear_kernal2.png)
Polynomial kernal (left) and radial kernal (right) (Hastie, et. al).

Our hyperplane equation becomes <br>
$$f(\overrightarrow{x}) = \overrightarrow{\alpha}^T K(x, x_i) + b$$

where $$\alpha_i \ne 0$$ are the weights vectors. 

See this [link][kernal_link]{:target = "_blank"} for an example of what a kernal does.

# Strengths and Weaknesses of SVM

**Strengths:**

* When classes are nearly separable, does better than logistic regression
* Can model nonlinear boundaries

**Weaknesses:**

* Unlike logistic regression and LDA/QDA, cannot estimate class probabiltiies

# Example
To perform SVM, one can use the package `e1071`
```{r, message = FALSE, warning = FALSE}
library(e1071)
```

```{r, echo = FALSE}
set.seed(3)
x <- matrix(rnorm(20*2), ncol=2)
y <- c(rep(-1,10), rep(1,10))
x[y==1,] = x[y==1,] + 1
lin_dat <- data.frame(x = x, y = as.factor(y))

set.seed(3)
x <- matrix(rnorm(200*2), ncol=2)
x[1:100,] = x[1:100,] + 2
x[101:150,] = x[101:150,] - 2
y <- c(rep(1,150),rep(2,50))
nonlin_dat <- data.frame(x = x, y = as.factor(y))
```

First, we can look at a linear SVM example. 
```{r, echo = FALSE}
ggplot(data = lin_dat, aes(x = x.1, y = x.2, color = y)) +
  geom_point(size = 1.5) +
  theme(legend.position = "none")
```

```{r}
# fit linear SVM
lin.svm.fit <- svm(y ~ ., data = lin_dat, kernel = "linear", cost = 10, scale = TRUE)

# look at fit
summary(lin.svm.fit)

# the support vectors
lin.svm.fit$index

# look at plot
plot(lin.svm.fit, lin_dat)
```
This plot isn't too appealing. It wouldn't be too hard to write our own. 

Next, we can take a look at the use of other kernals.
```{r, echo = FALSE}
ggplot(data = nonlin_dat, aes(x = x.1, y = x.2, color = y)) +
  geom_point(size = 1.5) +
  theme(legend.position = "none")
```

```{r}
# fit nonlinear SVM
nonlin.svm.fit <- svm(y ~ ., data = nonlin_dat, kernal = "radial", gamma = 10, cost = 1, scale = TRUE)

# look at fit
summary(nonlin.svm.fit)

# look at plot
plot(nonlin.svm.fit, nonlin_dat)
```

[kernal_link]: https://www.youtube.com/watch?v=3liCbRZPrZA
