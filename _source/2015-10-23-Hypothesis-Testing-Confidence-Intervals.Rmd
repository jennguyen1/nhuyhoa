---
layout: post
title: "Hypothesis Testing and Confidence Intervals"
date: "October 23, 2015"
categories: ['statistics', 'experimental design']
---

* TOC
{:toc}

```{r global opts, echo = FALSE, warning = FALSE}
library(jn.general)
lib(data, viz)
knitr::opts_chunk$set(fig.width = 5, fig.height = 5, fig.align = 'center')
```

# Hypothesis Testing
In hypothesis testing we attempt to test a whether a population parameter is significantly different than some predefined hypothesis value. 

The steps for conducting a hypothesis test is

1. Set up 2 competing hypotheses: $$H_0$$ and $$H_1$$
2. Set some significance level $$\alpha$$
3. Calculate test statistic
4. Calculate p-value 
5. Make a test decision & state overall conclusion

We have Type 1 and Type 2 errors with hypothesis testing. 

```{r, echo=FALSE}
c1 <- c("Decision", "Reject H0", "Accept H0")
c2 <- c("H0 is true", "Type 1 error", "Correct")
c3 <- c("H0 is false", "Correct", "Type 2 error")
data.frame(c1, c2, c3) %>% nhuyhoa_df_print(data = FALSE, col.names = c("", "Reality", ""))
```
<p></p>

If we reject $$H_0$$ when $$H_0$$ is true, we commit a Type I error. The probability of a Type 1 error is denoted $$\alpha$$.

If we accept $$H_0$$ when $$H_0$$ is false, we commit a Type II error. The probability of a Type II error is denoted $$\beta$$. 

## Derivation
Derivation of a test statistic is based off distributions of known values. Often we employ several theorems, such as the central limit theorem, which helps us to generate these statistics.

See individual tests for more information.

## Interpretation
Hypothesis tests generate a p-value. The p-value calculate may depend on the alternative hypothesis

* For right tailed tests, we compute $$P(X > X*)$$
* For left tailed tests, we compute $$P(X < X*)$$ 
* For two-sided tests, we compute $$2*P(X > \vert X* \vert)$$

The p-value is defined as the probability of obtaining a result greater than or more extreme than the one observed, given that the null hypothesis is not true. 

$$ p.value = P(X > x | H_0) $$

One can deduce (carefully) that since the result is unlikely to occur under the null hypothesis, the data had to have been generated from another probability distribution. 

There are a lot of potentially dangerous ways to interpret p-values. Assumptions should be assessed prior to drawing a final conclusion,  and multiple corrections should be adjusted for. P-values should be assessed, not just taken at face value.

# Confidence Intervals

## Derivation
Confidence intervals are used to convey the amount of uncertainty associated with a sample estimate of a population parameter. In other words, we want to obtain the shaded region in the diagram below. (For this example, we want to find the two-sided confidence interval. We can adjust these for one sided tests easily by removing one of the sides.)

```{r, echo = FALSE}
x <- seq(-3, 3, length.out = 100)
y <- dnorm(x)
xshade <- seq(-1.96, 1.96, length.out = 100)
shade <- data.frame(xs = c(-1.96, xshade, 1.96), ys = c(0, dnorm(xshade), 0))

ggplot(data = NULL, aes(x, y)) + 
  geom_line(size = 1.15) +
  geom_hline(yintercept = 0) + 
  geom_segment(aes(x = -1.96, xend = -1.96, y = 0, yend = dnorm(-1.96)), size = 1.15) +
  geom_segment(aes(x = 1.96, xend = 1.96, y = 0, yend = dnorm(1.96)), size = 1.15) +
  geom_polygon(data = shade, aes(x = xs, y = ys), alpha = 0.5, fill = "#0066ff", color = "black") +
  annotate("text", x = 0, y = 0.2, label = "1 - a", size = 7) +
  annotate("text", x = -2.25, y = 0.012, label = "a/2", size = 4.5) +
  annotate("text", x = 2.25, y = 0.012, label = "a/2", size = 4.5) + 
  ylab("") +
  theme(axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank())
```

To generate a $$(1-\alpha)100$$% confidence interval, we start with the test statistic. The test statistic may vary depending on its distribution, but the process is the same.

**Example 1:**
For this example, I will use the t-statistic, which we know is distributed $$t$$. We wish to obtain an interval around $$\bar{X}$$ which satisifies

$$P(-t_{\alpha/2} < \frac{\bar{X} - \mu}{SE} < t_{\alpha/2}) = 1 - \alpha$$

Let $$SE$$ be the standard error. So we have

$$-t_{\alpha/2} < \frac{\bar{X} - \mu}{SE} < t_{\alpha/2}$$

$$-t_{\alpha/2} * SE < \bar{X} < t_{\alpha/2} * SE$$

$$\mu - t_{\alpha/2} * SE < \bar{X} < \mu + t_{\alpha/2} * SE$$

So finally we have

$$\bar{X} \pm t_{\alpha/2} * SE$$

**Example 2:**
For this example, consider the confidence interval for the ratio of two variances $$\frac{\sigma^2_y}{\sigma^2_x}$$. We know that $$\frac{(n-1)s^2}{\sigma^2}$$ is distributed $$X^2_{n - 1}$$ and $$\frac{X^2_p / p}{X^2_q / q}$$ is distributed $$F_{p, q}$$. So we wish to compute

$$P(F_{p, q, 1 - \alpha/2} < \frac{\frac{(n - 1) s^2_x}{\sigma_x^2} / (n - 1)}{\frac{(m - 1) s^2_y}{\sigma_y^2} / (m - 1)} < F_{p, q, \alpha/2}) = 1 - \alpha$$

So we have

$$F_{p, q, 1 - \alpha/2} <\frac{\frac{(n - 1) s^2_x}{\sigma_x^2} / (n - 1)}{\frac{(m - 1) s^2_y}{\sigma_y^2} / (m - 1)} <F_{p, q, \alpha/2}$$

$$F_{p, q, 1 - \alpha/2} <\frac{\frac{s^2_x}{\sigma_x^2}}{\frac{s^2_y}{\sigma_y^2}} <F_{p, q, \alpha/2}$$

$$F_{p, q, 1 - \alpha/2} \frac{s^2_y}{s^2_x} < \frac{\sigma^2_y}{\sigma^2_x} < F_{p, q, \alpha/2}\frac{s^2_y}{s^2_x}$$


## Interpretation
Confidence intervals are used to convey the amount of uncertainty associated with a sample estimate of a population parameter. Data is collected and the results are used to generate a point estimate and a confidence interval. How does one interpret the confidence interval?

Say we generated a $$(1 - \alpha) 100$$% confidence interval. This interval means that if we were to replicate the data collection and analysis process many times, $$(1 - \alpha) 100$$% of the generated intervals would contain the true value of the population parameter. 

```{r, echo = FALSE}
set.seed(22)
btn <- Vectorize(between)
# create the data
x <- data.frame(id = factor(1:100), center = rnorm(100), width = runif(100, 0.25, 2))
x <- x %>% 
  mutate(
    lower = center - width,
    upper = center + width,
    flag = ifelse( btn(0, lower, upper), "no_color", "color" )
  )

# edits to generate the 95%
y <- to_be(x, subset, flag == "color")
edit <- y$to_be
no_edit <- y$not_to_be

# only keep 5 outside of 0 bars
n <- nrow(edit) - 5
keep <- edit[1:5, ]
more_keep <- sample_n(no_edit, n, replace = TRUE)

# combine all data together
final_data <- list(no_edit, keep, more_keep) %>% 
  rbindlist %>% 
  sample_n(., nrow(.)) %>% 
  mutate(id = 1:nrow(.))
```

Say the true value of $$\mu = 0$$. Here we see, per the definition above, that $$95/100$$ intervals contain the true value of $$\mu$$. 

```{r, echo = FALSE}
# make the plot
ggplot(data = final_data, aes(x = id, ymax = lower, ymin = upper, color = flag)) + 
  geom_errorbar() + 
  # add the baseline
  geom_hline(yintercept = 0, linetype = 2, alpha = 0.75) + 
  # color the bars
  scale_colour_manual(limits = c("color", "no_color"), values = c("red", "grey50")) + 
  # x scale
  xlab("") + scale_x_discrete(breaks = NULL) +
  # remove legend position
  theme(legend.position = "none") +
  # flip the coordinates
  coord_flip() +
  # title
  ggtitle("95% Confidence Interval Simulation")
```

It is important to note that the true value of the population parameter is fixed, so any single confidence interval will either contain the population parameter or not. Thus we cannot interpret the confidence interval as the probability that the true value is in the interval. 

So say we want to test a null hypothesis that $$\mu = 0$$ with a 95% confidence interval. If $$\mu$$ falls within that confidence interval, we say that if we were to repeat the process many times, $$95$$% of the time $$\mu$$ will fall inside the confidence intervals. Thus we can conclude that our data does not indicate a significant difference from $$\mu = 0$$. 